# Pull-only, pull-through, caching OCI Distribution Server

This chart installs a **pull-only**, **pull-through**, **caching** OCI Distribution server. That means:

1. It exclusively provides _pull_ capability. You can't push images to it, it doesn't support the `/v2/_catalog` endpoint, etc. Though you can pre-load it.
2. It provides *caching pull-through* capability to any upstream registry: internal, air-gapped, or public; supporting the following types of access: anonymous, basic auth, HTTP, HTTPS, one-way TLS, and mTLS.

This OCI distribution server is intended to satisfy one use case: the need for a Kubernetes caching pull-through registry that enables a k8s cluster to run reliably in disrupted, disconnected, intermittent and low-bandwidth (DDIL) edge environments. (However, it also nicely mitigates rate-limiting issues when doing local Kubernetes development.)

## Quick Start (three steps)

## 1) Install the chart

```shell
CHARTVER=n.n.n
helm upgrade --install ociregistry oci://quay.io/appzygy/helm-charts/ociregistry\
  --version $CHARTVER\
  --namespace ociregistry\
  --create-namespace\
  --dry-run=server
```

> Remove the `--dry-run=server` arg to actually perform the install.

By default the chart will create a `NodePort` service on port `31080` in your cluster for `containerd` to mirror to. (This is configurable via a values override.)

## 2) Configure `containerd`

Configure containerd in your Kubernetes cluster to mirror **all** image pulls to the pull-through registry. (This has been tested with containerd >= `v1.7.6`):

First, add a `config_path` entry to `/etc/containerd/config.toml` to tell containerd to load all registry mirror configurations from that directory:

```shell
   ...
   [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = "/etc/containerd/certs.d"
   ...
```

Then create a configuration file that tells containerd to pull from the caching pull-through registry server in the cluster. This is an example for `_default` which indicates to containerd that **all** images should be mirrored:

```shell
mkdir -p /etc/containerd/certs.d/_default && \
cat <<EOF >| /etc/containerd/certs.d/_default/hosts.toml
[host."http://localhost:31080"]
  capabilities = ["pull", "resolve"]
  skip_verify = true
EOF
```

**Key Points:**

1. The _resolve_ capability tells containerd that a HEAD request to the server with a manifest will return a manifest digest. The _pull_ capability indicates to containerd that the image can be pulled.
2. Assuming you installed the caching pull-through OCI registry with the default `NodePort` service option on port `31080`, every host on the cluster will route `31080` to the Pod running the registry.

The `containerd` daemon _should_ detect the change and re-configure itself. If you believe that's not occurring, then `systemctl restart containerd`.

## 3) Verify

### Tail the logs on the pull-through registry pod:

```shell
kubectl -n ociregistry logs -f -l app.kubernetes.io/name=ociregistry
```

### Observe the startup logs

```shell
time="2025-04-24T21:11:41Z" level=info msg="loaded 0 manifest(s) from the file system in 102.136µs"
----------------------------------------------------------------------
OCI Registry: pull-only, pull-through, caching OCI Distribution Server
Version: vZ.Z.Z, build date: 2025-04-23T00:35:28.79Z
Started: 2025-04-24 21:11:41.727569108 +0000 UTC (port 8080)
Running as (uid:gid) 65532:65532
Process id: 1
Tls: none
Command line: /ociregistry/server --config-file /var/ociregistry/config/registry-config.yaml serve
----------------------------------------------------------------------
time="2025-04-24T21:11:41Z" level=info msg="server is running"
```

### Run `hello-world` container

```shell
kubectl run hello-world --image docker.io/hello-world:latest &&\
  sleep 5s &&\
  kubectl logs hello-world
```

### Result

```shell
Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
```
(remainder redacted for brevity...)

### Observe the **new** ociregistry log entries

```shell
time="2025-04-24T21:13:54Z" level=info msg="pulling manifest from upstream: \"docker.io/library/hello-world:latest\""
time="2025-04-24T21:13:54Z" level=info msg="echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=501.131286ms host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:13:54Z" level=info msg="serving manifest from cache: \"docker.io/library/hello-world@sha256:c41088499908a59aae84b0a49c70e86f4731e588a737f1637e73c8c09d995654\""
time="2025-04-24T21:13:54Z" level=info msg="echo server GET:/v2/library/hello-world/manifests/sha256:c410884999?ns=docker.io status=200 latency=2.267101ms host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:13:54Z" level=info msg="pulling manifest from upstream: \"docker.io/library/hello-world@sha256:03b62250a3cb1abd125271d393fc08bf0cc713391eda6b57c02d1ef85efcc25c\""
time="2025-04-24T21:13:55Z" level=info msg="echo server GET:/v2/library/hello-world/manifests/sha256:03b62250a3?ns=docker.io status=200 latency=612.467708ms host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:13:55Z" level=info msg="echo server GET:/v2/library/hello-world/blobs/sha256:74cc54e27d?ns=docker.io status=200 latency=234.948µs host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:13:55Z" level=info msg="echo server GET:/v2/library/hello-world/blobs/sha256:e6590344b1?ns=docker.io status=200 latency=232.113µs host=localhost:31080 ip=10.200.0.232"
```

### Run `hello-world` Pod again with a different name

This time, also specify a pull policy to force `containerd` to pull the image:

```shell
kubectl run hello-world-2 --image docker.io/hello-world:latest --image-pull-policy=Always
```

### Observe new ociregistry log entries

These entries indicate that the _ociregistry_ server is serving from the image cache instead of re-pulling from Docker Hub:

```shell
time="2025-04-24T21:34:35Z" level=info msg="serving manifest from cache: \"docker.io/library/hello-world:latest\""
time="2025-04-24T21:34:35Z" level=info msg="echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=204.413µs host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:34:36Z" level=info msg="serving manifest from cache: \"docker.io/library/hello-world:latest\""
time="2025-04-24T21:34:36Z" level=info msg="echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=358.099µs host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:34:51Z" level=info msg="serving manifest from cache: \"docker.io/library/hello-world:latest\""
time="2025-04-24T21:34:51Z" level=info msg="echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=313.859µs host=localhost:31080 ip=10.200.0.232"
time="2025-04-24T21:35:09Z" level=info msg="serving manifest from cache: \"docker.io/library/hello-world:latest\""
time="2025-04-24T21:35:09Z" level=info msg="echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=836.294µs host=localhost:31080 ip=10.200.0.232"
```

## TLS

By default the server serves over HTTP in the cluster. To serve over HTTPS, the chart has the `serverTls` hash. To enable TLS:

1. Have server cert and key PEM files, and optionally a CA PEM file, on the file system.
2. Configure values like this:
   ```yaml
   serverTls:
     enabled: true
     clientAuth: none # or 'verify'
   ```
3. Deploy the chart:
   ```shell
   helm upgrade --install ociregistry oci://...\
    --namespace ociregistry\
    --create-namespace\
    --values <file with contents above>.yaml\
    --set-file serverTls.cert=/tmp/localhost.crt\
    --set-file serverTls.key=/tmp/localhost.key\
    --set-file serverTls.ca=/tmp/localhost.crt
   ```

If you already have a secret in the cluster with cert, key, and (optionally) ca then configure the `serverTls` as shown below. Then omit supplying values/files for the `tls.crt`, `tls.key`, and `ca.crt` sub-keys of the `serverTls` hash. For example, suppose:

```shell
kubectl -n ociregistry create secret generic my-tls-secret\
  --from-file=tls.crt=/tmp/localhost.crt\
  --from-file=tls.key=/tmp/localhost.key\
  --from-file=ca.crt=/tmp/localhost.crt
```

Then you would install the ociregistry with this a values override containing this:

```yaml
serverTls:
  secretName: my-tls-secret
  enabled: true
  clientAuth: verify # or 'none'
```

And you helm install command would look like this (omit the `--set-file` args):

```shell
helm upgrade --install ociregistry oci://...\
  --namespace ociregistry\
  --create-namespace\
  --values <file with contents above>.yaml
```

## Monitoring and metrics

Metrics can be enabled as follows. First enable observability. This creates the Prometheus and Grafana resources, including dashboard `ConfigMaps` for Grafana:
```yaml
observability:
  enabled: true
```

> Since these resources are created in the `ociregistry` namespace, you will need to configure Prometheus to find Service Monitors and Prometheus Rules in that namespace, or all namespaces.

Next enable metrics exposition by defining a metrics port for the server to expose metrics on. There are two ways:

Using the `metrics` value:
```yaml
metrics:
  port: <port num>
```

Or, using the `serverConfig` value:
```yaml
serverConfig:
  configuration:
    metrics: <port num>
```

## Volumetrics

The server stores each image manifest in memory twice - once by digest and once by tag for fast retrieval. The 64-byte blob digests are also cached in memory. Each image manifest is a different size, and each image has a different number of blobs. So there is no such thing as a typical cache configuration. If you consider 5K as a hypothetical average manifest size and approximately 5 as a hypothetical average number of blobs per image, then the amount of memory required to cache 1,500 image manifests and blob digests in memory is less than 10Mb.

However, the bulk of the server's work in the steady state is serving blobs. Serving blobs requires a lot of file system reads. This _can_ result in the OS caching a large volume of file pages in memory. This OS cache is **not** visible to the go runtime and can consume the entire host memory. The typical scenario in which this occurs is when a large number of pods request the same set of images, resulting in the server reading the same blobs from the file system concurrently.

Consider the following Prometheus graph of the `container_memory_working_set_bytes` metric. In this example a load test began at 02:27 and continued until about 02:34. During this period the load test pulled the same 50 images from multiple clients. The images comprising the test ranged in size from a few hundred bytes to several hundred megabytes. The effect of a large number of server goroutines reading the same set of file system pages caused the OS to significantly increase the amount of memory used to cache filesystem pages. The Ociregistry server under test was configured with `resources.limits.memory: 300Mi`. The metric floor of about 25Mi is all Go-managed memory that Ociregistry needs to manage its in-memory manifest and blob digest cache. Everything else is OS file page caching:

![load-test.jpg](https://raw.githubusercontent.com/aceeric/ociregistry/main/charts/ociregistry/load-test.jpg)

Because of OS file page caching, you are **strongly** recommended to configure memory limits in your values. The following resource configuration will allow plenty of memory for the Go heap objects that comprise the cache, and the the OS caching overhead that is not visible to the go runtime. Testing has shown that the OS will honor the pod memory limit and trim the file page cache to live within the pod limit and avoid OOMKilling the pod. Example (for the hypothetical 1,500 images discussed above):

```yaml
resources:
  requests:
    cpu: 500m
    memory: 300Mi
  limits:
    cpu: "1"
    memory: 300Mi
```

## More information

More information, including how to configure access to upstream registries for authentication and TLS, as well as additional registry features and capabilities can be found at: https://github.com/aceeric/ociregistry.

## Chart Details

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

## Chart Values

{{ template "chart.valuesSection" . }}

{{ template "helm-docs.versionFooter" . }}