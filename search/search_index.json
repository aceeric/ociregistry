{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":""},{"location":"#ociregistry","title":"Ociregistry","text":"<p>Ociregistry is a pull-only, pull-through, caching OCI Distribution server. That means:</p> <ol> <li>It exclusively provides pull capability. It does this by implementing a subset of the OCI Distribution Spec.</li> <li>It provides caching pull-through capability to multiple upstream registries: internal, air-gapped, or public; supporting the following types of access: anonymous, basic auth, HTTP, HTTPS (secure &amp; insecure), one-way TLS, and mTLS. In other words, one running instance of this server can simultaneously pull from <code>docker.io</code>, <code>quay.io</code>, <code>registry.k8s.io</code>, <code>ghcr.io</code>, your air-gapped registries, in-house corporate mirrors, etc.</li> </ol>"},{"location":"#goals","title":"Goals","text":"<p>The goal of the project is to build a performant, simple, reliable edge OCI Distribution server for Kubernetes. One of the overriding goals was simplicity: only one binary is needed to run the server, and all state is persisted as simple files on the file system under one subdirectory. This supports the following use cases for running Kubernetes:</p> <ol> <li>Edge clusters.</li> <li>Air-gapped clusters - loading the server in a connected environment, and then serving a cluster in an air gap.</li> <li>Offloading corporate OCI Registries to minimize load / dependency on corporate resources.</li> <li>Running the registry in-cluster as a Kubernetes workload, and then mirroring containerd to the registry within the cluster or across clusters.</li> <li>Supporting development environments. For example, I do a lot of Kubernetes experimentation at home. In my home environment I run small multi-VM Kubernetes clusters on my desktop. I run Ociregistry on my desktop as a systemd service and mirror <code>containerd</code> in my dev Kubernetes clusters to the systemd service.</li> <li>Avoiding rate-limiting, but also being a good internet citizen by lightening the load on the large (free) distribution servers provided to us by DockerHub and many others.</li> </ol> <p>Other distribution servers index for availability and fault tolerance at the cost of increased complexity. This is not a criticism - it is simply a fact of design trade-offs. This server indexes for simplicity and accepts the availability and fault tolerance provided by the host.</p>"},{"location":"#summary","title":"Summary","text":"<p>The goals of the project are:</p> <ol> <li>Implement a narrow set of use cases, mostly around serving as a caching mirror for Kubernetes clusters.</li> <li>Be simple, performant, and reliable.</li> </ol>"},{"location":"airgap-considerations/","title":"Airgap Considerations","text":"<p>As stated in the Overview, one objective of the project is to support running Kubernetes in air-gapped and DDIL environments. To accomplish this, you will likely adopt one of two approaches:</p> <ol> <li>Stand the cluster up in a connected environment, pre-load the distribution server with all required images, then disconnect and ship the cluster to its edge location.</li> <li>Ship the cluster to its edge location and pre-load the distribution server there using stable comms. If comms are later degraded or lost then the required images remain cached.</li> </ol> <p>There are two general ways to run the distribution server in support of this objective. These are now discussed.</p>"},{"location":"airgap-considerations/#kubernetes-workload","title":"Kubernetes Workload","text":"<p>Using the helm chart described elsewhere in this guide you can install the server as a Kubernetes <code>Deployment</code>, and configure containerd on each Node to access the distribution server on a <code>NodePort</code>. In order to make this work, two things are needed:</p> <ol> <li>The cluster needs persistent storage that is redundant and reliable. This means that when you down the cluster, ship it, and start it up at the edge, the persistent storage has to come up with the image cache intact.</li> <li>You need a tarball of the distribution server container image available in the edge environment for the following reason: when you start the cluster for the first time, if containerd has been configured to pull from the distribution server, the distribution server image may not be in cache and so that Pod won't start. This is a classic deadlock scenario. Therefore your cluster startup procedure will be to load the distribution server image tarball into each cluster's containerd cache before starting the cluster. Now Kubernetes can start the distribution server workload which in turn can serve images from its cache. Ideally your Kubernetes distribution of choice will support the ability to pre-load containerd from an image tarball at a configurable file system location.</li> </ol>"},{"location":"airgap-considerations/#systemd-service","title":"<code>systemd</code> Service","text":"<p>You can run the distribution server as a <code>systemd</code> service on one of the cluster nodes. This avoids the deadlock scenario associated with running the distribution server as a Kubernetes workload because the service will come up when the cluster instances are started and will therefore immediately be available to serve cached images.</p> <p>The risk is that the image cache only exists on one cluster instance. If this instance is lost, then the cluster is down until comms are re-established. This can be mitigated by replicating the cache across multiple cluster nodes. There are multiple tools available to support this. For example Syncthing could be run on each node to ensure that each node has a full copy of the image cache. If the node running the distribution server is lost then the nodes can have their containerd configuration modified to point to any one of the other nodes, and the distribution server systemd service can be started on that node.</p>"},{"location":"auth/","title":"Authentication","text":"<p>By default, Ociregistry authenticates with the upstream registry if required using the built-in auth mechanisms of the OCI distribution spec implemented by the upstream server itself.</p> <p>External authentication is also supported. External authentication means obtaining an auth token from an external endpoint, and then using the auth token to pull from the upstream.</p>"},{"location":"auth/#how-it-works","title":"How it works","text":"<p>When you configure a registry for external auth, Ociregistry obtains an auth token from that external auth mechanism and uses that token to pull from the upstream:</p> <pre><code>sequenceDiagram\n    Ociregistry-&gt;&gt;+Auth Service: Request token\n    Auth Service--&gt;&gt;-Ociregistry: Token\n    Ociregistry-&gt;&gt;+Upstream: Pull with Token</code></pre> <p>Auth config is part of the <code>registry</code> configuration element in the <code>config.yaml</code> provided to Ociregistry on the command line: <code>ociregistry --config-file somefile.yaml serve</code>. The registry configuration looks as follows. The <code>token</code> key introduces external auth: <pre><code>registries:\n  - name: my-registry.io\n    auth:\n      token:\n        provider: &lt;provider name&gt;\n        providerOpts: &lt;comma-separated key=value provider-specific opts&gt;\n        expiry: &lt;expiration duration&gt;\n        static: &lt;literal token value&gt;\n</code></pre></p> <p>The <code>token</code> structure is detailed below:</p> Field Description <code>provider</code> Currently, <code>ecr</code> is the only supported provider. See the next section for details on ECR support. <code>providerOpts</code> These are provider-specific comma-separated key/value pairs. E.g.: <code>foo=bar,bin=baz</code>. <code>expiry</code> This is the token expiration period. The Ociregistry will run a background goroutine to refresh the token every <code>expiry</code> period of time. Valid time units are <code>s</code> (seconds), <code>m</code> (minutes), and <code>h</code> (hours). E.g.: <code>1h</code>. These values are supported by the Go duration parser. You are free to specify a shorter value than the actual token expiration, for example to ensure that you never have an expired token. <code>static</code> This was introduced for testing but remains as a fall-back. If you obtain a token, you can paste it in here, start the server, and the server will use this rather than running a goroutine to refresh the token periodically."},{"location":"auth/#amazon-elastic-container-registry-ecr","title":"Amazon Elastic Container Registry (ECR)","text":"<p>ECR token-based auth gets a token the same way as this AWS CLI command: <pre><code>aws ecr get-authorization-token --output text\\\n  --query 'authorizationData[].authorizationToken'\n</code></pre></p> <p>Ociregistry uses the AWS SDK to obtain the token. The SDK obtains its IAM identity and entitlements from the AWS configuration of the host (EC2 instance, Kubernetes pod, laptop, bare metal server, etc.) Fundamentally Ociregistry doesn't know how the AWS SDK obtains the token. It is the responsibility of the registry installer, or DevOps team to configure the processor that runs Ociregistry to have an identity and entitlement to get a token. From the Ociregistry server's perspective, all those concerns are opaque. Clearly though, the requirement is for Ociregistry to be running on a host with an AWS account context.</p> <p>As long as <code>aws sts get-caller-identity</code> returns something meaningful on the processor that is running the Ociregistry server, then the ECR integration should work.</p> <p>The following provider opts are currently supported:</p> Option Description <code>profile</code> A profile name in <code>~/.aws/credentials</code>. <code>region</code> An AWS region. E.g. <code>us-south-1</code>. <p>Here is a sample <code>registry</code> configuration entry for ECR integration:</p> <pre><code>registries:\n  - name: 111122223333.dkr.ecr.us-south-1.amazonaws.com\n    auth:\n      token:\n        provider: ecr\n        providerOpts: profile=default,region=us-south-1\n        expiry: 6h\n</code></pre> <p>The provider opts are probably redundant in the example above. The goal is to express what is allowed.</p>"},{"location":"command-line/","title":"The Command Line","text":"<p>The command line parser uses the urfave/cli parser. Running the server with no arguments shows the following sub-commands:</p> <pre><code>NAME:\n   ociregistry - a pull-only, pull-through, caching OCI distribution server\n\nUSAGE:\n   ociregistry [global options] [command [command options]]\n\nCOMMANDS:\n   serve    Runs the server\n   load     Loads the image cache\n   list     Lists the cache as it is on the file system\n   prune    Prunes the cache on the filesystem (server should not be running)\n   version  Displays the version\n   help, h  Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --log-level string    Sets the minimum value for logging: debug, warn, info, or error (default: \"error\")\n   --config-file string  A file to load configuration values from (cmdline overrides file settings)\n   --image-path string   The path for the image cache (default: \"/var/lib/ociregistry\")\n   --log-file string     log to the specified file rather than the console\n   --help, -h            show help\n</code></pre> <p>The simplest way to run the  server with all defaults is:</p> <pre><code>ociregistry serve\n</code></pre> <p>Each sub-command also supports help, as expected. E.g.: <code>ociregistry serve --help</code></p>"},{"location":"concur-design/","title":"Concurrency Design","text":"<p>The server maintains an in-memory cache of all manifests on the file system. Access to the manifest cache is synchronized via the in-memory replica because each pull updates the manifest pull date/time both on the file system and in memory. This supports the ability to prune the cache by pull recency.</p>"},{"location":"concur-design/#image-in-cache","title":"Image in cache","text":"<p>In the simple case, where a manifest is in cache, if multiple clients pull concurrently, the goroutines serving the clients are briefly synchronized to get the manifest from cache and update the pull timestamp. Each goroutine then exits the synchronization block and runs concurrently to serve the image. In the diagram, there is a blue goroutine and a red goroutine running concurrently. In the illustration, the blue goroutine is first into the synchronization block. (Time elapses in the rightward direction.)</p> <p></p> <p>In the example, the red goroutine will be the last one to update the image pull timestamp.</p>"},{"location":"concur-design/#image-not-in-cache","title":"Image not in cache","text":"<p>If the image is not in cache, it is a bit more complex. The complexity is introduced by the design goal to make efficient use of the network. So rather than having multiple concurrent pulls from the upstream, if multiple goroutines simultaneously request the same image that is not in cache, only one goroutine will actually perform the pull, and all other (concurrent) goroutines will wait for that image.</p> <p>Here is how that looks:</p> <p></p> <ol> <li>As in the previous example, both goroutines are run concurrently by a client such as containerd. As before, each goroutine is sychronized when querying the image cache. But in this example, the image is not in cache.</li> <li>The blue goroutine is first so when it exits the synchronization block it initiates a pull from the upstream.</li> <li>The red goroutine is second to pull the non-existing image so when it exits the synchronization block it is parked by the server, waiting for the blue goroutine to finish pulling the image. So the amount of time for two (or ten, or more) clients to concurrently request an image that needs to be pulled is about the same for all the goroutines. The difference is - only the pulling goroutine will actually go to the upstream and utilize the network.</li> <li>When the blue goroutine finishes the pull, it adds the image to the cache, which is synchronized. When it exits the synchronization block it signals all goroutines waiting for this particular image - signified by the star in the diagram. This un-parks the red goroutine. The blue goroutine continues on to serve the image asynchronously.</li> <li>The red goroutine accesses the image from the cache, sychronized.</li> <li>The red goroutine exits the synchronization block and serves the image asynchronously.</li> </ol> <p>Pull synchronization (from upstreams) is by image tag, and is separate from the synchronization for images in cache. In other words, ten clients pulling hello-world:v3 because it is un-cached and ten other clients pulling hello-world:v2 that is un-cached and ten other clients accessing hello-world:v1 from cache are three disjoint synchronization contexts that don't effect each other.</p>"},{"location":"concur-design/#blob-synchronization","title":"Blob synchronization","text":"<p>The concurrency design for blobs is a little different. The reason is that there is no need to update pull timestamps on blobs to support image pruning since blobs are children of image manifests. When a manifest is pruned the manifest blobs can simply be pruned by reference. Whereas a manifest is updated each time it is pulled, a blob is never updated. Therefore blobs are synchronized with a Go RWMutex.</p> <p>This enables many concurrent pulls with very minimal sycnhronization overhead, but supports the infrequent addition of blobs resulting from new image pulls, and the removal of blobs as a result of image pruning. When blobs are added because of new pulls, or removed because of pruning, this will temporarily lock the blob cache. The server offers optimization techniques for this. For example, when configuring background pruning, you can configure Ociregistry to prune small sets with greater frequency.</p> <p>For blobs, only the digest is cached in memory. This is used to efficiently determine if the blob is cached. Whereas manifests are served from memory, blobs are served from the file system.</p>"},{"location":"configuring-the-server/","title":"Configuring The Server","text":"<p>The server will accept configuration on the command line, or via a configuration yaml file. Most of the configuration file settings map directly to the command line. The exceptions are the <code>pruneConfig</code>, <code>registries</code>, and <code>serverTlsConfig</code> entries which are only accepted in the configuration file at this time, being too complex to easily represent as command line args.</p> <p>As one would expect the following values provide configuration with the lowest priority on the bottom and the highest priority on the top:</p> <pre><code>block\n  columns 1\n  a[\"Command line\"]\n  b[\"Config file\"]\n  c[\"Hard-coded defaults in the Go code\"]</code></pre> <p>To provide full configuration in a file, run the server this way:</p> <pre><code>bin/ociregistry --config-file &lt;some file&gt; serve\n</code></pre> <p>But this is also valid:</p> <pre><code>bin/ociregistry --config-file &lt;some file&gt; serve --port 9999\n</code></pre>"},{"location":"configuring-the-server/#defaults","title":"Defaults","text":"<p>The yaml document below shows a <code>config.yaml</code> file specifying the defaults that are implemented in the Go code. In other words, running the server with this exact configuration file is the same as running with no configuration file:</p> <pre><code>imagePath: /var/lib/ociregistry\nlogLevel: error\nlogFile:\npreloadImages:\nimageFile:\nport: 8080\n#os: omit to use the server OS\n#arch: omit to use the server architecture\npullTimeout: 60000\nalwaysPullLatest: false\nairGapped: false\nhelloWorld: false\ndefaultNs:\nhealth:\nmetrics:\nregistries: []\npruneConfig:\n  enabled: false\nserverTlsConfig: {}\n</code></pre>"},{"location":"configuring-the-server/#config-file-keys-and-values","title":"Config file keys and values","text":"Key Type Default Command line arg Description <code>imagePath</code> Path spec /var/lib/ociregistry <code>--image-path</code> The base path for the image cache. The server will create sub-directories under this for blobs and manifests. <code>logLevel</code> keyword error <code>--log-level</code> Error level logging. See help for valid values. <code>logFile</code> Path spec - <code>--log-file</code> Empty means log to stderr. If you specify a file, the logging is directed to the file. <code>preloadImages</code> Path spec - <code>--preload-images</code> If the <code>serve</code> subcommand is specified, then this is the file containing a list of images to pre-load before starting the server. <code>imageFile</code> Path spec - <code>--image-file</code> If the <code>load</code> subcommand is specified, then this is the file containing a list of images to load. <code>port</code> Integer 8080 <code>--port</code> The port to serve on <code>os</code> keyword runtime.GOOS <code>--os</code> If loading or preloading, the OS and arch. If empty, then defaults to the host running the server. So usually comment these out. <code>arch</code> keyword runtime.GOARCH <code>--arch</code> \" <code>pullTimeout</code> Integer 60000 <code>--pull-timeout</code> Number of milliseconds before a pull from an upstream will time out. <code>alwaysPullLatest</code> Boolean false <code>--always-pull-latest</code> If true, then whenever a latest tag is pulled, the server will always pull from the upstream - in other words it acts like a basic proxy. Useful when supporting dev environments where latest is frequently changing. <code>airGapped</code> Boolean false <code>--air-gapped</code> If true, will not attempt to pull from an upstream when an image is requested that is not cached. <code>helloWorld</code> Boolean false <code>--hello-world</code> For testing. Only serves 'docker.io/hello-world:latest' from embedded blobs and manifests <code>defaultNs</code> String Empty <code>--default-ns</code> Allows pulling without an explicit namespace. Otherise, a namespace is required either in-path (<code>docker pull ociregistry:8080/docker.io/hello-world</code>) or as a query param the way <code>containerd</code> does it when registry mirroring is configured in the <code>containerd</code> <code>config.toml</code>. E.g. if <code>--default-ns=docker.io</code> then <code>docker pull ociregistry:8080/hello-world</code> will pull from <code>docker.io</code>, otherwise it is an error. <code>health</code> Integer - <code>--health</code> A port number to run a <code>/health</code> endpoint on for Kubernetes liveness and readiness. By default, the server doesn't listen on a health port. The Helm chart enables this by default when running the server as a cluster workload. <code>metrics</code> Integer - <code>--metrics</code> A port number to run a <code>/metrics</code> endpoint on for Prometheus. By default, the server doesn't enable or expose metrics. <code>registries</code> List of dictionary <code>[]</code> n/a Upstream registries configuration. See further down for registry configuration. <code>pruneConfig</code> Dictionary see below n/a Prune configuration. Pruning is disabled by default. See further down for prune configuration. <code>serverTlsConfig</code> Dictionary <code>{}</code> n/a Configures TLS with downstream (client) pullers, e.g. containerd. By default, serves over HTTP. See server tls configuration further down."},{"location":"configuring-the-server/#registry-configuration","title":"Registry Configuration","text":"<p>The OCI Distribution server may need configuration to connect to upstream registries. If run with no configuration for a given registry, the server will default to anonymous insecure <code>HTTPS</code> access. You specify registry configuration using the <code>registries</code> list:</p> <pre><code>registries:\n- name: upstream one\n  description: foo\n  scheme: https\n  auth: {}\n  tls: {}\n- name: upstream two\n  description: bar\n  scheme: http\n  auth: {}\n  tls: {}\n- etc...\n</code></pre> <p>Each entry supports the following configuration structure overall. Not all values are required. More detail is presented on that below. This shows the full structure:</p> <pre><code>- name: my-upstream # or my-upstream:PORT, e.g. index.docker.io\n  description: Something that makes sense to you (or omit it - it is optional)\n  scheme: https # (the default), also accepts http\n  auth:\n    user: theuser\n    password: thepass\n    passwordFromEnv: THEPASS_ENV_VAR\n    token: {}\n  tls:\n    ca: /my/ca.crt\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: true/false # defaults to false\n</code></pre> <p>Since <code>scheme</code> defaults to <code>https</code> you can omit that entirely. The <code>tls</code> key is optional. If omitted and <code>scheme</code> is https then the Ociregistry server attempts insecure 1-way TLS. The default for <code>tls.insecureSkipVerify</code> is <code>false</code> if omitted (and <code>tls</code> is specified.) Similarly, <code>description</code> is ignored by the server and can be omitted.</p> <p>The <code>auth</code> section implements basic auth, just like your <code>~/.docker/config.json</code> file.</p> <p>Note</p> <p>The <code>auth.token</code> configuration element supports token-based authentication using an external service and is documented in the Authentication section.</p> <p>The <code>name</code> value is the upstream host name that you will pull from. For example say you're running the server on your desktop in test mode on <code>localhost:8080</code>. Let's also say that you have an in-house corporate registry that serves proprietary images on <code>my-corp-registry.myco.org:8888</code>. You run this command to pull through the Ociregistry server from your corporate registry server this way: <pre><code>docker pull localhost:8080/my-corp-registry.myco.org:8888/my-proprietary-app:v1.0.0\n</code></pre> Say your corp registry server requires user and password. Then you would configure a <code>registries</code> entry in the Ociregistry server config file like this: <pre><code>- name: my-corp-registry.myco.org:8888\n  auth:\n    user: theuser\n    password: thepass\n</code></pre></p> <p>To read the password from an environment variable, use the <code>passwordFromEnv</code> key: <pre><code>- name: my-corp-registry.myco.org:8888\n  auth:\n    user: theuser\n    passwordFromEnv: THEPASS_ENV_VAR\n</code></pre></p> <p>Here's another scenario. Let's say your corp DNS resolves <code>index.docker.io</code> to an in-house registry mirror that requires an NPE cert. Then your registry entry might look like: <pre><code>- name: index.docker.io\n  tls:\n    cert: npe.crt\n    key: npe.key\n</code></pre></p> <p>The <code>tls</code> section can implement multiple scenarios:</p> <ol> <li>One-way insecure TLS, in which client certs are not provided to the remote, and the remote server cert is not validated:    <pre><code>tls:\n  insecureSkipVerify: true\n</code></pre></li> <li>One-way secure TLS, in which client certs are not provided to the remote, and the remote server cert is validated using the OS trust store:    <pre><code>tls:\n  insecureSkipVerify: false # (or omit, since it defaults to false)\n</code></pre></li> <li>One-way secure TLS, in which client certs are not provided to the remote, and the remote server cert is validate using a provided CA cert:    <pre><code>tls:\n  ca: /my/ca.crt\n</code></pre></li> <li>mTLS (client certs are provided to the remote):    <pre><code>tls:\n  cert: /my/client.cert\n  key: /my/client.key\n</code></pre>    mTLS can be implemented with and without remote server cert validation as described above in the various one-way TLS scenarios. Examples:    <pre><code>- name: foo.bar.1.io\n  description: mTLS, don't verify server cert\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: true\n- name: foo.bar.2.io\n  description: mTLS, verify server cert from OS trust store\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: false\n- name: foo.bar.3.io\n  description: mTLS, verify server cert from provided CA\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    ca: /remote/ca.crt\n    insecureSkipVerify: false\n</code></pre></li> </ol>"},{"location":"configuring-the-server/#server-tls-configuration","title":"Server TLS configuration","text":"<p>By default, the server serves over HTTP. The <code>serverTlsConfig</code> section in the config file configures the Ociregistry server to serve over TLS. </p> <p>The <code>cert</code>, <code>key</code>, and <code>ca</code> are expected to be PEM encoded files. Example:</p> <pre><code>serverTlsConfig:\n  cert: /path/to/pem/encoded/server.crt\n  key: /path/to/pem/encoded/server.private.key\n  ca: /path/to/pem/encoded/ca.crt\n  clientAuth: none # or verify\n</code></pre>"},{"location":"configuring-the-server/#server-tls-permutations","title":"Server TLS permutations","text":"<p>The following permutations are supported or serving over HTTPS:</p> Configuration Description <code>cert</code> and <code>key</code> populated The server will provide the cert to the client to establish 1-way TLS. <code>clientAuth: none</code> Client cert is not requested and will be ignored if provided. <code>clientAuth: verify</code> mTLS: Client cert is required and verified. See the <code>ca</code> key below. <code>ca</code> populated Client cert is validated against the provided CA. <code>ca</code> omitted Client cert is validated against the OS trust store."},{"location":"configuring-the-server/#prune-configuration","title":"Prune Configuration","text":"<p>Pruning configures the server to remove images as a background process based on create date or recency of a pull. (Each time an image is pulled the server updates the pull date/time for the image.) Pruning is disabled by default. An example full prune configuration is as follows:</p> <pre><code>pruneConfig:\n  enabled: true\n  duration: 30d\n  type: accessed\n  frequency: 1d\n  count: -1\n  dryRun: false\n</code></pre>"},{"location":"configuring-the-server/#prune-configuration-keys-and-values","title":"Prune configuration keys and values","text":"Key Type Description <code>enabled</code> Boolean If true, enables background pruning. <code>duration</code> Duration expression E.g.: <code>30d</code>. The value is interpreted based on the prune <code>type</code> below. Valid time units are <code>ns</code> (nanoseconds), <code>us</code> or <code>\u00b5s</code> (microseconds), <code>ms</code> (milliseconds), <code>s</code> (seconds), <code>m</code> (minutes), <code>h</code> (hours), and <code>d</code> (days). <code>type</code> Keyword Valid values are <code>accessed</code> and <code>created</code>. If <code>accessed</code>, then the server prunes images that have not been pulled in <code>duration</code> amount of time. If <code>created</code>, then the server prunes images whose create date is older than <code>duration</code> time ago. <code>frequency</code> Duration expression E.g.: <code>1d</code>. Run the background pruner with this frequency. The time units are the same as for <code>duration</code>. <code>count</code> Integer The number of images to prune on each run of the background pruner. A value of <code>-1</code> means no limit to the number of images pruned. <code>dryRun</code> Boolean If <code>true</code> then just log messages but don't actually prune. For testing and troubleshooting. <p>Since pruning locks the cache, a good strategy is to limit the number of pruned images on each invocation of the pruner and run with greater frequency.</p>"},{"location":"design/","title":"Design","text":"<p>This is the design of the Ociregistry server:</p> <p></p> <p>Narrative:</p> <ol> <li>A client (in this case - <code>containerd</code>) initiates an image pull. The image pull consists of a series of REST API calls. The API calls are handled by the server's implementation of a subset of the OCI Distribution Spec.</li> <li>The API is just a veneer that delegates to the server implementation.</li> <li>The server checks the local cache and if the image is in cache it is immediately returned from cache.</li> <li>If the image is not in cache, the server calls the embedded ImgPull library to pull the image from the upstream registry. The server knows which upstream to pull from because <code>containerd</code> appends a query parameter (e.g. <code>?ns=registry.k8s.io</code>) to each API call. <p>The server also supports in-path upstreams, e.g.: <code>docker pull ociregistry.host/registry.k8s.io/pause:3.8</code></p> </li> <li>The embedded image puller library pulls the image from the upstream registry and returns it to the server.</li> <li>The server adds the image to cache and returns the image to the caller from the newly updated cache.</li> </ol>"},{"location":"design/#image-pull-sequence-diagram","title":"Image Pull Sequence Diagram","text":"<p>By way of background, a typical image pull sequence is as follows. If you tail the logs of the Ociregistry server, this is what you'll see:</p> <pre><code>sequenceDiagram\n    Client-&gt;&gt;Server: HEAD the manifest list by tag, e.g. \"registry.k8s.io/pause:3.8\"\n    Server--&gt;&gt;Client: Digest of the manifest list in a response header (or 404 Not Found)\n    Client-&gt;&gt;Server: GET the manifest list by digest\n    Server--&gt;&gt;Client: Send a manifest list listing all available manifests\n    Client-&gt;&gt;Client: Pick an image manifest digest from the manifest list matching the desired OS and architecture\n    Client-&gt;&gt;Server: GET the image manifest by digest \n    Server--&gt;&gt;Client: Send the image manifest\n    Client-&gt;&gt;Server: GET the blobs for the image\n    Server--&gt;&gt;Client: Send the blobs</code></pre> <p>To support this, the server caches both the image list manifest and the image manifest.</p>"},{"location":"design/#code-structure","title":"Code Structure","text":"<p>The source code is organized as shown:</p> <pre><code>project root\n\u251c\u2500\u2500 api\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 cmd\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 impl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 auth\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cache\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cmdline\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 globals\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 helpers\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 metrics\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 preload\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pullrequest\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serialize\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers.go\n\u2502   \u2514\u2500\u2500 ociregistry.go\n\u2514\u2500\u2500 mock\n</code></pre> Package Description <code>api</code> Mostly generated by <code>oapi-codegen</code> using the OAPI Spec <code>ociregistry.yaml</code> in that directory. <code>bin</code> Has the compiled server after <code>make server</code>. <code>charts</code> The Helm chart. <code>cmd</code> Entry point (<code>ociregistry.go</code>) and sub-commands. <code>docs</code> MKDocs documentation (Material theme.) <code>impl</code> Has the implementation of the server. <code>impl/auth</code> Implements external authentication (i.e. Amazon ECR.) <code>impl/cache</code> Implements the in-memory cache. <code>impl/cmdline</code> Parses the command line. <code>impl/config</code> Has system configuration. <code>impl/globals</code> Globals. <code>impl/helpers</code> Helpers. <code>impl/metrics</code> The Observability implementation. <code>impl/preload</code> Implements the load and pre-load from an image list file. <code>impl/pullrequest</code> Abstracts the URL parts of an image pull. <code>impl/serialize</code> Reads/writes from/to the file system. <code>impl/handlers.go</code> Has the code for the subset of the OCI Distribution Server API spec that the server implements. <code>impl/ociregistry.go</code> A veneer that the embedded Echo server calls that simply delegates to <code>impl/handlers.go</code>. See the next section - REST API Implementation for some details on the REST API. <code>mock</code> Runs a mock upstream OCI Distribution server used by the unit tests."},{"location":"design/#rest-api-implementation","title":"REST API Implementation","text":"<p>As stated above, the Ociregistry server implements a portion of the OCI Distribution Spec consisting of only the endpoints in the spec needed to meet its goal of being a pull-only OCI Distribution Server. It does this by running an http server that handles REST endpoints defined in the spec.</p> <p>The Ociregistry server REST API is built by first creating an Open API spec: see ociregistry.yaml in the <code>api</code> directory of the project. Then the oapi-codegen tool is used to generate the API code and the Model code using configuration in the <code>api</code> directory of the project. This approach was modeled after the OAPI-Codegen Petstore example.</p> <p>Oapi-codegen is installed by the following command:</p> <pre><code>go install github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@latest\n</code></pre> <p>The key components of the API scaffolding supported by OAPI-Codegen are shown below:</p> <pre><code>\u251c\u2500\u2500 api\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500models.gen.go   (generated)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models.cfg.yaml    (modeled from pet store)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ociregistry.gen.go (generated)\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 server.cfg.yaml    (modeled from pet store)\n\u251c\u2500\u2500 cmd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ociregistry.go     (this is the server - which embeds the Echo server)\n\u2514\u2500\u2500 ociregistry.yaml       (the openapi spec built with swagger)\n</code></pre> <p>I elected to use the Echo option to run the API. The Echo server is started by the serve sub-command of the Ociregistry server.</p>"},{"location":"limitations/","title":"Limitations","text":"<ol> <li>The server serves from an in-memory cache of the image store, synchronized using Go synchronization objects (mutexes.) Therefore only one instance of the server can serve clients. So in Kubernetes, for example, you can't run multiple replicas behind a service.</li> <li>The maximim number of path segments supported is for an image is four. E.g. <code>docker pull ociregistry.host:8080/ghcr.io/two/three/four:v1.1.1</code>. In the example, <code>ghcr.io</code> is the upstream namespace, taking up one of the four segments, and leaving <code>two/three/four</code> as the full repository name. When containerd mirrors to Ociregistry, it appends the namespace as a query parameter on the REST API calls, which frees up an additional segment which would allow an image name like <code>one/two/three/four:v1.1.1</code> to be mirrored.</li> </ol>"},{"location":"load-testing/","title":"Load Testing","text":"<p>The project includes tooling in the testing/load directory to conduct two types of load tests. One load test focuses on concurrent pulls from upstreams and the second focuses on concurrent pulls from cache. Both tests use the same tools and test driver.</p> <p>To test concurrent pulls from upstreams, the test driver pulls through the Ociregistry and then immediately prunes all the pulled images, repeating this in a loop. To test concurrent cached pulls the process is the same, omitting the pruning step.</p>"},{"location":"load-testing/#physical-topology","title":"Physical Topology","text":"<p>The following diagram shows the physical processors used in the tests documented in this section:</p> <p></p> <ol> <li>The test driver is a Ubuntu laptop with 32 gigs of memory and Intel Core i7-8700SH x16 processors. It runs the test driver CLI.</li> <li>The server is a Ubuntu workstation with 64 gigs of memory and Intel Core i7-8700 x12 processors. It runs the Ociregistry server as a stand-alone executable, and Docker Registry in a container run by Docker daemon (containerd.)</li> <li>Both test driver and server machines are on the same network segment.</li> </ol>"},{"location":"load-testing/#software-components","title":"Software Components","text":"<p>The following project components implement load testing:</p> <p></p> <p>Some of the shell scripts shown use a tool called imgpull which is both a CLI and a library. It's the component that the Ociregistry embeds as a package to pull from upstreams.</p>"},{"location":"load-testing/#prepare","title":"Prepare","text":"<ol> <li>The <code>observability</code> directory has a shell script <code>start-containers</code> that starts Prometheus and Grafana in containers, mounting all the configuration files needed to monitor Ociregistry metrics using the provided dashboards.</li> <li>The <code>testing/load</code> directory has the following shell scripts:<ol> <li>The <code>maketar</code> script generates many image tarballs with randomized image references like <code>zhymakpdjr-wt379wo54x-0001:v883.998</code>. The first part (<code>zhymakpdjr-wt379wo54x</code>) and the tag (<code>:v883.998</code>) promote uniqueness across the entire test set to be able to generate a large volume of images without name collisions. The middle part (<code>-0001</code>) is to enable batching. Batching enables the test driver to task one goroutine with pulling images matching <code>-0001</code>, the second goroutine pulling <code>-0002</code>, and so on. More on batching below.</li> <li>The <code>load-docker-container</code> script starts the Docker Registry in a container, loads image tarballs into the Docker cache, tags them, and then pushes them to the Docker Registry running in the container. At this point, the test is ready to run.</li> </ol> </li> </ol>"},{"location":"load-testing/#execute","title":"Execute","text":"<ol> <li>The <code>testing/load/driver</code> directory actually runs the tests. See the Test Driver section details.</li> <li>When the test driver starts, it first queries the Registry container for all images (that were loaded by the <code>load-docker-container</code> script.)</li> <li>The test driver then pulls from Ociregistry and records the pull rate for the duration of the test.</li> <li>And of course the Ociregistry server under test is run on the server, to pull through on <code>:8080</code>, and exposing metrics on <code>:2112/metrics</code>. Both ports are configurable. The <code>/metrics</code> path is not.</li> </ol>"},{"location":"load-testing/#test-driver","title":"Test Driver","text":"<p>The test driver scales up - and then down - a number of goroutines to pull from the Ociregistry server concurrently and records the client-side image pull rate. This includes pulling the image and blobs and creating a tarball. (There's some overhead here, granted.) The observability stack records server behavior during the test, and then the results are evaluated. The test driver records the client's area of concern: how many images can be pulled per second? Fundamentally, this is what <code>containerd</code> will care about.</p> <p></p>"},{"location":"load-testing/#patterns-and-batching","title":"Patterns and Batching","text":"<p>When testing pull-through, the test driver supports running each goroutine with an exclusive set of images using a pattern. The idea of patterns is to chunk the image list into disjoint sets and thereby force a high level of concurrency for pull-through. To elaborate further: if 100 clients were to pull exactly the same image at exactly the same instant from Ociregistry, then only one client will actually pull and the other 99 will be parked by the server and then pull from cache when the first client finishes pulling from the upstream and adds the image to cache.</p> <p>Because of this design, having many clients pull the same exact image concurrently doesn't really test pull-through concurrency. (It does test cached pull concurrency.) Having many clients pull disjoint images concurrently actually tests pull-through concurrency (and load.)</p> <p>To support this the test driver supports two modes. If the <code>--prune</code> arg is specified then each goroutine will prune the images it pulled on each pass. On the next pass through for that goroutine, the Ociregistry will have to re-pull from the upstream again.  If the <code>--prune</code> arg is not specified then each goroutine will simply be pulling from cache which will measure a different behavior in the server.</p> <p>Note that since pruning itself introduces locking and load on the server it distorts the pull-through test somewhat but there's no other way to test a large volume of concurrent pulls without loading the upstream with a significant number of images.</p>"},{"location":"load-testing/#test-preparation","title":"Test Preparation","text":"<p>The preparation steps are:</p> <ol> <li>Run the <code>maketar</code> script to generate a set of images. These are small images that test concurrency. Each image has five small blobs. So this isn't about testing large blob pulls, network latency, etc. It's purely a concurrency test. The <code>maketar</code> script has logic to generate a defined number of batches. For example: 1000 images in batches of 100 with the batching pattern in the image ref like <code>-0001</code>, <code>-0002</code> and so on.</li> <li>Start the Docker Registry container on port 5000.</li> <li>Run the <code>load-docker-container</code> script to move the image tarballs into the Docker Registry container.</li> <li>Start Ociregistry server on port 8080, exposing metrics on port 2112, on the <code>/metrics</code> path.</li> <li>Start Prometheus and Grafana.</li> </ol>"},{"location":"load-testing/#test-execution","title":"Test Execution","text":"<ol> <li> <p>Start the test driver with args that support the batching strategy employed when the <code>maketar</code> script was run.</p> <ol> <li>To test pull through, specify the <code>--prune</code> arg.</li> <li>To test cached pulls, omit the <code>--prune</code> arg.</li> </ol> <p>Example (cached pull test): <pre><code>go run .\\\n  --pullthrough-url=ubuntu.me:8888\\\n  --registry-url=ubuntu.me:5000\\\n  --patterns=-0001,-0002,-0003\\\n  --iteration-seconds=30\\\n  --tally-seconds=15\n</code></pre></p> </li> <li> <p>The test driver first scales up the goroutines with each goroutine pulling one unique batch (if configured that way).</p> </li> <li>The test driver then scales the goroutines down to zero, which ends the test.</li> <li>The test driver generates pull rate metrics either to a file or to the console depending on command line args.</li> </ol> <p>I considered capturing test driver metrics with Prometheus. Instead, I thought there might be value in a second opinion on calculating the pull rate - especially from the client's perspective. So the test driver metrics are implemented with Golang packages, namely the atomic and ticker packages.</p>"},{"location":"load-testing/#results","title":"Results","text":"<p>The test results documented below capture a test run with the following parameters:</p> <ol> <li>Docker Registry populated with 1,500 images each with 5 small blobs.</li> <li>Ten batches of 150 images each, enabling 10 client puller goroutines to pull disjoint sets.</li> <li>Test driver scales from 1 to 10 puller goroutines and then back down to 1, then stops the test.</li> <li>Test driver runs for 60 seconds between adding / stopping puller goroutines resulting in a 20 minute test.</li> </ol> <p>Each result section below stacks the charts listed in the table below. Since client side metrics were calculated using go (not prometheus) and imported into Excel, the first chart looks different than the Grafana dashboard panels. The reader is directed to the <code>impl/metrics</code> package for details on the go runtime metrics.</p> Where Metric Note Client side test driver Image pulls per second and puller Goroutines These are full downloads of an image tar to the file system. Ociregistry server Detailed memory statistics Ociregistry server Goroutines and go threads Ociregistry server Heap Bytes Ociregistry server Heap Objects Ociregistry server Garbage collection cycles Ociregistry server Mutex wait The server synchronizes shared cache access with a mutex. Ociregistry server Network send/receive bytes Ociregistry server Topline resident memory Ociregistry server Manifest pull rate Usually each image pull will consist of at least one image list manifest pull and one image manifest pull. Ociregistry server Blob pull rate The test arbitrarily sets up 5 small blobs per image Ociregistry server Cached or Upstream pull rate by namespace Ociregistry server API Error rate"},{"location":"load-testing/#pull-through-results","title":"Pull-Through Results","text":"<p>The pull-through test pulled through the Ociregistry server to the Docker registry. After each pass through the pull list, the puller goroutine pruned the images it just pulled, forcing the Ociregistry to re-pull them the next time. The prune operation - while  fast - does lock the in-mem cache briefly which certainly impacts performance and therefore taints the test. However, without having the ability to load an upstream registry with literally hundreds of thousands of images, the selected approach was the only feasible way to test pull-through concurrency.</p> <p>Topline summary: with 10 client goroutines, the client-side pull rate topped out at about 320 images per second. The rate never peaked so its possible that with more goroutines, a higher pull rate could be acheived. Some aspect of latency is attributable to the \"upstream\" Docker Registry container but that was not specifically isolated.</p> <p> Test driver: Puller goroutines scaling up from 1 to 10 then back down.</p> <p> Go runtime detailed mem stats. The Green line is memory classes total usage bytes. Yellow (top) is memory classes total bytes (sum of all). The green line is the most relevant</p> <p> Go runtime. Yellow is go threads, green is goroutines.</p> <p> Go runtime. Yellow is heap released bytes, green is heap alloc bytes.</p> <p> Go runtime heap objects. When the server starts this spikes is associated with loading the in-memory cache on startup.</p> <p> Go runtime garbage collection cycles.</p> <p> Go runtime mutex wait time/sec.</p> <p> Go runtime network bytes. Yellow is send. Green is receive.</p> <p> Go runtime process memory.</p> <p> Ociregistry manifest pull rate (includes image list manifests and image manifests.)</p> <p> Ociregistry blob pulls: 5 blobs and a config blob for each image pulled.</p> <p> Ociregistry: cached manifest pull.</p> <p> Ociregistry: no API errors.</p>"},{"location":"load-testing/#cached-pull-results","title":"Cached Pull Results","text":"<p>The cached pull simply pulls from cache (without pruning.)</p> <p>Topline summary: with 10 client goroutines, the client-side pull rate topped out at about 1,800 images per second. Interestingly as the pull load hit 10 puller goroutines, the server suffered some performance degradation. I attribute this to the high mutex wait which is a side-effect of the simple concurrency design which forces all pulls through a mutex so that each pull can update the pull date/time stamp to support pruning by pull recency.</p> <p> Test driver: Puller goroutines scaling up from 1 to 10 then back down.</p> <p> Go runtime detailed mem stats. The Green line is memory classes total usage bytes. Yellow (top) is memory classes total bytes (sum of all). The green line is the most relevant</p> <p> Go runtime. Yellow is go threads, green is goroutines.</p> <p> Go runtime. Yellow is heap released bytes, green is heap alloc bytes.</p> <p> Go runtime heap objects. When the server starts there is a spike in heap objects associated with loading the in-memory cache on startup.</p> <p> Go runtime garbage collection cycles.</p> <p> Go runtime mutex wait time/sec.</p> <p> Go runtime network bytes. Yellow is send. Green is receive.</p> <p> Go runtime process memory.</p> <p> Ociregistry manifest pull rate (includes image list manifests and image manifests.)</p> <p> Ociregistry blob pulls: 5 blobs and a config blob for each image pulled.</p> <p> Ociregistry: cached manifest pull.</p> <p> Ociregistry: no API errors.</p>"},{"location":"load-testing/#summary","title":"Summary","text":"<p>The load test is an artificial effort to max out concurrency. The test set consists of 1,500 small images. Pulling any one of these images produces a roughly 9K tarball that can be pulled from cache in 7.5 milliseconds:</p> <pre><code>$ imgpull localhost:8888/ubuntu.me:5000/a3xuzdp7kd-pmwvzztrob-0010:v339.283 deleteme.tar --scheme http\nimage \"localhost:8888/a3xuzdp7kd-pmwvzztrob-0010:v339.283\" saved to \"deleteme.tar\" in 7.487947ms\n</code></pre> <p>Then: <pre><code>$ ls -l deleteme.tar\n-rw-rw-r-- 1 foo foo 9216 Dec  5 21:10 deleteme.tar\n</code></pre></p> <p>Given the huge variance in image sizes, network latency, etc. in the real world, I venture to say a real world load test would be hard to devise. But from the current load test, the project draws the small number of conclusions. (It is worth re-stating the design goals of the server: to be simple, reliable, and performant.)</p> <ol> <li>The server is reliable. Every image tarball requested by the test driver was returned. The server doesn't appear to leak memory or goroutines.</li> <li>The server is performant in its role as an edge distribution sever. Even with 1,500 manifests in the in-mem cache, the memory footprint (&lt; 40Mb) doesn't seem extreme.</li> <li>The server is reasonably simple in its operational footprint and design. One clear downside of the simple concurrency design is the dip in throughput under the heaviest load. However, it seems unlikely that the server would ever be loaded in the wild as heavily as it was loaded by the test. Therefore, there seems no compelling reason (other than curiosity) for pursuing design optimizations.</li> </ol>"},{"location":"loading/","title":"Loading And Pre-Loading","text":"<p>Loading and Pre-loading supports the air-gapped use case of populating the registry in a connected environment, and then moving it into an air-gapped environment.</p> <p>You can pre-load the cache two ways:</p> <ol> <li>As a startup task before running the service: <code>bin/ociregistry serve --preload-images &lt;file&gt;</code>. The server will load the image cache and then serve.</li> <li>By using the binary as a CLI: <code>bin/ociregistry load --image-file &lt;file&gt;</code>. The executable will load the cache and then exit back to the command prompt.</li> </ol> <p>In both cases, you create a file with a list of image references. Example:</p> <pre><code>cat &lt;&lt;EOF &gt;| imagelist\nquay.io/jetstack/cert-manager-cainjector:v1.11.2\nquay.io/jetstack/cert-manager-controller:v1.11.2\nquay.io/jetstack/cert-manager-webhook:v1.11.2\nregistry.k8s.io/metrics-server/metrics-server:v0.6.2\nregistry.k8s.io/ingress-nginx/controller:v1.8.1\nregistry.k8s.io/pause:3.8\ndocker.io/kubernetesui/dashboard-api:v1.0.0\ndocker.io/kubernetesui/metrics-scraper:v1.0.9\ndocker.io/kubernetesui/dashboard-web:v1.0.0\nEOF\n</code></pre> <p>Since the entirety of the image cache consists of files and sub-directories under the image cache directory, you can tar that directory up at any time, copy it somewhere, untar it, and start an Ociregistry server instance there pointing to the copied directory and it will just work.</p>"},{"location":"loading/#image-store","title":"Image Store","text":"<p>The image store is persisted to the file system. This includes blobs and manifests. Let's say you run the server with <code>--image-path=/var/lib/ociregistry</code>, which is the default. Then:</p> <pre><code>/var/lib/ociregistry\n\u251c\u2500\u2500 blobs\n\u251c\u2500\u2500 img\n\u2514\u2500\u2500 lts\n</code></pre> <ol> <li><code>blobs</code> are where the blobs are stored.</li> <li><code>img</code> stores the non-<code>latest</code>-tagged image manifests.</li> <li><code>lts</code> stores the <code>latest</code>-tagged image manifests. (See About \"Latest\" below.)</li> </ol> <p>Everything is stored by digest. When the server starts it loads everything into an in-memory representation. Each new pull through the server while it is running updates both the in-memory representation of the image store as well as the persistent state on the file system.</p> <p>The software uses a data structure called a ManifestHolder to hold all the image metadata and the actual manifest bytes from the upstream registry. These are simply serialized to the file system as JSON. (So you can find and inspect them if needed for troubleshooting with <code>grep</code>, <code>cat</code>, and <code>jq</code>.)</p> <p>A <code>ManifestHolder</code> looks like this: <pre><code>type ManifestHolder struct {\n    Type                 ManifestType\n    Digest               string\n    ImageUrl             string\n    Bytes                []byte\n    V1ociIndex           v1oci.Index\n    V1ociManifest        v1oci.Manifest\n    V2dockerManifestList v2docker.ManifestList\n    V2dockerManifest     v2docker.Manifest\n    Created              string\n    Pulled               string\n}\n</code></pre></p> <p>The <code>Bytes</code> field has the actual manifest bytes from the upstream. You can see the supported manifest types: <code>V1ociIndex</code>, <code>V1ociManifest</code>, <code>V2dockerManifestList</code>, and <code>V2dockerManifest</code>.</p>"},{"location":"loading/#loading-behavior","title":"Loading behavior","text":"<p>Loading is additive, meaning if you run the load command to load 100 images, then run it again to load 100 different images, your image cache will have 200 images. If you load 100 images, and then later load the same 100 images again, the server will detect during the second load that there is nothing to do. And of course, if you first load A, B, and C, and then later load C, D, and E, then the cache will hold A, B, C, and D.</p>"},{"location":"loading/#about-latest","title":"About \"Latest\"","text":"<p>Internally, <code>latest</code>-tagged images are stored side-by-side with non-latest images and treated as separate manifests. This enables the server to support cases that occur in development environments where <code>latest</code> images are in a constant state of flux. Storing <code>latest</code> images this way works in tandem with the <code>--always-pull-latest</code> flag as follows:</p> Action <code>--always-pull-latest</code> Result Pull <code>foo:latest</code> <code>false</code> (the default) The image is pulled exactly once. All subsequent pulls return the same image regardless of what happens in the upstream. Pull <code>foo:latest</code> <code>true</code> The image is pulled from the upstream on each pull from the pull-through server for each client. Each pull completely replaces the prior pull. In other words - for latest images the server is a stateless proxy. (This could consume a fair bit of network bandwidth.)"},{"location":"observability/","title":"Observability","text":"<p>The Ociregistry server implements Prometheus metrics exposition. Metrics are implemented in the metrics package of the server. Two types of Metrics are provided in two Grafana dashboards, discussed in the following sections.</p>"},{"location":"observability/#metrics-dashboards","title":"Metrics Dashboards","text":""},{"location":"observability/#go-runtime-metrics","title":"Go Runtime Metrics","text":"<p>Go runtime metrics make use of the Go runtime/metrics package. The implementation of Go runtime metrics in the server was informed by the following articles, for which the project expresses appreciation:</p> <ol> <li>Monitor Your Go Process Internal Metrics in Minutes by Gil Adda, CyberArk Engineering</li> <li>Go memory metrics demystified by Datadog</li> </ol> <p>The Go runtime metrics package consists of a large set of measurements. All the metrics are exposed to Prometheus, but only the subset that seemed most relevant to the server are presented in the project's Go runtime metrics dashboard.</p>"},{"location":"observability/#ociregistry-metrics","title":"Ociregistry Metrics","text":"<p>The Ociregistry metrics are a narrower set than the Go Runtime metrics, reflecting the simple nature of the server:</p> Metric Description Cached Pulls By Namespace This is a running total of pulls of cached manifests. The dash presents it as a rate. These are bucketed by namespace (e.g. <code>docker.io</code>, <code>quay.io</code>, and so on. Upstream Pulls By Namespace This is a running total of manifest pulls from upstream registries. Also presented as a rate, and bucketed. Manifest Pulls Total Simply the sum of cached and un-cached pulls. Blob Pulls Like manifest pulls, this is the count of blob pulls. Since most manifests contain many blobs, this is expected to be a larger number than the sum of cached and un-cached pulls. Blob Bytes On Disk Total blob bytes on the file system. Manifest Bytes On Disk Total manifest bytes on the file system. Cached Manifest Count Number of manifests in the in-mem cache. Since most images you pull consist of a multi-arch image list manifest, and an os/arch-specific image, this count will always be greater than the number of manifest files on the file system. The goal here is to understand memory footprint. Cached Blob Count Same as cached manifest count. However, since only the blob digest is cached in-mem, and only cached once, this should match the file system blob count. V2 Api Endpoint Hits Total hits against the V2 OCI Distribution Server spec endpoints that are implemented by the server. Api Errors This is the count of ant API call that results in an error. For example, if one client undertakes an image pull and starts requesting the blobs for an image, and another client simultaneously prunes that image and blobs, then the first client may request a blob that is no longer cached. This is handled as an error by the server."},{"location":"observability/#how-to-use","title":"How to use","text":"<p>Metrics are disabled in the server by default, and can be enabled for the <code>serve</code> sub-command with either the <code>--metrics</code> command-line arg, or the <code>metrics:</code> key in the config file. E.g.: <code>ociregistry serve --metrics 2112</code>.</p> <p>Or: <code>ociregistry --config-file mycfg.yaml serve</code>, if <code>mycfg.yaml</code> has: <pre><code>metrics: 2112\n</code></pre></p> <p>Metrics can be exposed on any port, but take note that the observability directory of the project has <code>2112</code> hard-coded into the Prometheus scrape configuration so be aware of that when starting the metrics observability components discussed in the next section.</p>"},{"location":"observability/#observability-components","title":"Observability Components","text":"<p>As discussed above, the observability directory of the project has the items needed to access the metrics. The main entry point there is the <code>start-containers</code> Bash script.</p> <p>This script starts Prometheus, scraping on port 2112 on localhost and then starts Grafana, pulling from Prometheus on localhost:9090. At the present moment, the metrics are only available when running the Ociregistry server locally. Primarily they support load testing.</p>"},{"location":"observability/#dashboards","title":"Dashboards","text":""},{"location":"observability/#go-runtime-metrics-dashboard","title":"Go Runtime Metrics dashboard","text":"<p>From top left to right, and then down, the dashboard shows:</p> Row Left Panel Right Panel 1 Memory Goroutines and threads 2 Heap bytes Heap objects 3 Garbage collection Mutex wait 4 GC cycles Process memory <p>Process memory is distorted by Virtual Mem which may be coming out of the panel after further evaluation.</p>"},{"location":"observability/#ociregistry-metrics-dashboard","title":"Ociregistry Metrics dashboard","text":"<p>From top left to right, and then down, the dashboard shows:</p> Row Left Panel Right Panel 1 Manifest pulls Blob pulls 2 Cached pulls Upstream (un-cached) pulls 3 File system blob and manifest bytes Cached manifest and blob counts 4 Total endpoint hits API errors <p>There are a couple of things to be aware of regarding manifest statistics:</p> <ol> <li>The dashboard counts manifests in the in-memory cache. Most images cached by the server consist of two manifests: a multi-platform image list manifest and an OS+architecture image manifest. Further, when a manifest is cached in memory by tag, it is stored twice: once by tag and once by digest. Because of this the count of cached manifests - being a memory count - will always be higher than the number of manifest files on the file system. The thing to remember is that the dashboard metrics are memory object counters.</li> <li>The manifest pull rate also reflects the fact that a typical client image pull consists of first an image list manifest pull (by tag), then an image manifest pull (by digest.) Therefore, the stats you see in the dashboard will almost always be higher a client-centric count of image pulls.</li> </ol>"},{"location":"observability/#summary","title":"Summary","text":"<p>In summary, to access the metrics:</p> <ol> <li><code>ociregistry serve --metrics 2112</code>.</li> <li><code>observability/start-containers</code>.</li> <li>Open Grafana in your browser on http://localhost:3000. (You might have to disable Tracking Protection in Firefox or other browser equivalent.)</li> <li>Log in with the user/pass in the shell script.</li> <li>The Ociregistry dashboards are in the <code>Ociregistry</code> folder under <code>Dashboards</code> in the Grafana UI left panel.</li> </ol>"},{"location":"pruning/","title":"Pruning The Image Cache","text":"<p>The compiled server binary can be used as a CLI to prune the cache on the file system.</p> <p>The server can also be configured to prune as a continual background process. For information on that, see Prune Configuration in the Configuring the Server page. There is also a REST API for ad-hoc pruning the running server's cache. See the Administrative REST API document for information on that.</p> <p>This page is about ad-hoc manual pruning using the server binary as a CLI.</p> <p>Important</p> <p>The server must be stopped while pruning using the binary as a CLI because the CLI only manipulates the file system, not the in-memory representation of the cache. Pruning removes manifest lists, manifests, and possibly blobs (more on blobs below.)</p>"},{"location":"pruning/#list-images","title":"List Images","text":"<p>Generally, it is expected that you will use the server binary as a CLI to list the images before deciding which images to prune. E.g.:</p> <pre><code>bin/ociregistry --image-path /my/image/cache list --pattern dashboard\n</code></pre> <p>Result (for example) truncated in the doc for readability:</p> <pre><code>docker.io/kubernetesui/dashboard-web@sha256:05ad8120...\ndocker.io/kubernetesui/dashboard-metrics-scraper@sha256:0cdefa04...\ndocker.io/kubernetesui/dashboard:v2.7.0\ndocker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\ndocker.io/kubernetesui/dashboard-api:1.12.0\ndocker.io/kubernetesui/dashboard-web:1.6.2\ndocker.io/kubernetesui/dashboard-auth:1.2.4\ndocker.io/kubernetesui/dashboard@sha256:ca93706e...\ndocker.io/kubernetesui/dashboard-auth@sha256:d6dd67b7...\ndocker.io/kubernetesui/dashboard-api@sha256:dcc897f8...\n</code></pre>"},{"location":"pruning/#dry-run","title":"Dry Run","text":"<p>It is strongly recommended to use the <code>--dry-run</code> arg to develop your pruning expression. Then remove <code>--dry-run</code> to actually prune the cache. When <code>--dry-run</code> is specified, the CLI shows you exactly what will be pruned but does not actually modify the file system.</p>"},{"location":"pruning/#by-pattern","title":"By Pattern","text":"<p>Specify <code>--pattern</code> with single parameter consisting of one or more manifest URL patterns separated by commas. The patterns are Golang regular expressions as documented in the regexp/syntax package documentation. The expressions on the command line are passed directly to the Golang <code>regex</code> parser as received from the shell, and are matched to manifest URLs. As such, shell expansion and escaping must be taken into consideration. Simplicity wins the day here. Examples:</p> <pre><code>bin/ociregistry prune --pattern kubernetesui/dashboard:v2.7.0 --dry-run\nbin/ociregistry prune --pattern docker.io --dry-run\nbin/ociregistry prune --pattern curl,cilium --dry-run\n</code></pre>"},{"location":"pruning/#by-create-datetime","title":"By Create Date/time","text":"<p>The <code>--prune-before</code> option accepts a single parameter consisting of a local date/time in the form <code>YYYY-MM-DDTHH:MM:SS</code>. All manifests created before that time stamp will be selected. Example:</p> <pre><code>bin/ociregistry prune --date 2024-03-01T22:17:31 --dry-run\n</code></pre> <p>The intended workflow is to use the CLI with <code>list</code> sub-command to determine desired a cutoff date and then to use that date as an input to the <code>prune</code> sub-command.</p>"},{"location":"pruning/#important-to-know-about-pruning","title":"Important to know about pruning","text":"<p>Generally, but not always, image list manifests have tags, and image manifests have digests. This is because in most cases, upstream images are multi-architecture. For example, this command specifies a tag:</p> <pre><code>bin/ociregistry prune --pattern calico/typha:v3.27.0 --dry-run\n</code></pre> <p>In this case, on a Linux/amd64 machine running the server the CLI will find two manifests:</p> <pre><code>docker.io/calico/typha:v3.27.0\ndocker.io/calico/typha@sha256:eca01eab...\n</code></pre> <p>The first is a multi-arch image list manifest, and the second is the image manifest matching the OS and Architecture that was selected for download. In all cases, only image manifests have blob references. If your search finds only an image list manifest, the CLI logic will also look for cached image manifests (and associated blobs) for the specified image list manifest since that's probably the desired behavior. (The blobs consume the storage.)</p>"},{"location":"pruning/#blob-removal-when-pruning","title":"Blob removal when pruning","text":"<p>Internally, the CLI begins by building a blob list with ref counts. As each image manifest is removed its referenced blobs have their count decremented. After all manifests are removed, any blob with zero refs is also removed. Removing an image manifest therefore won't remove blobs that are still referenced by un-pruned manifests.</p>"},{"location":"quickstart-desktop/","title":"Quick Start (Desktop)","text":"<p>The easiest way to understand how the server works is to git clone the repo, then build and run the server locally on your Linux desktop.</p> <p>After git-cloning the project:</p>"},{"location":"quickstart-desktop/#build-the-server","title":"Build the server","text":"<pre><code>make server\n</code></pre> <p>This command compiles the server and creates a binary called <code>ociregistry</code> in the <code>bin</code> directory relative to the project root.</p>"},{"location":"quickstart-desktop/#run-the-server","title":"Run the server","text":"<p>You provide a file system location for the image cache with the <code>--image-path</code> arg. If the directory doesn't exist the server will create it. The default is <code>/var/lib/ociregistry</code> but to kick the tires it makes more sense to use the system temp directory. By default the server listens on <code>8080</code>. If you have something already running and bound to that port, specify <code>--port</code>. We'll specify it explicitly here with the default value:</p> <pre><code>bin/ociregistry --image-path /tmp/images serve --port 8080\n</code></pre> <p>Result: <pre><code>----------------------------------------------------------------------\nOCI Registry: pull-only, pull-through, caching OCI Distribution Server\nVersion: 1.9.6, build date: 2025-10-06T23:35:46.56Z\nStarted: 2025-10-06 19:49:57.169081368 -0400 EDT (port 8080)\nRunning as (uid:gid) 1000:1000\nProcess id: 27010\nTls: none\nCommand line: bin/ociregistry --image-path /tmp/images serve --port 8080\n----------------------------------------------------------------------\n</code></pre></p>"},{"location":"quickstart-desktop/#curl-an-image-manifest-list","title":"Curl an image manifest list","text":"<p>Curl a manifest list using the OCI Distribution Server REST API. Note the <code>ns</code> query parameter in the URL below which tells the server to go to that upstream if the image isn't already locally cached. This is exactly how containerd does it when you configure containerd to mirror:</p> <pre><code>curl localhost:8080/v2/kube-scheduler/manifests/v1.29.1?ns=registry.k8s.io | jq\n</code></pre> <p>(The server also supports in-path namespaces like <code>localhost:8080/v2/registry.k8s.io/kube-scheduler/manifests/v1.29.1</code>)</p> <p>Result: <pre><code>  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n  \"manifests\": [\n    {\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"size\": 2612,\n      \"digest\": \"sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\",\n      \"platform\": {\n        \"architecture\": \"amd64\",\n        \"os\": \"linux\"\n      }\n    },\n    etc...\n</code></pre></p>"},{"location":"quickstart-desktop/#curl-an-image-manifest","title":"Curl an image manifest","text":"<p>Pick the first manifest from the list above - the <code>amd64/linux</code> manifest - and curl the manifest by SHA, again using the OCI Distribution Server REST API:</p> <pre><code>DIGEST=019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\ncurl localhost:8080/v2/kube-scheduler/manifests/sha256:$DIGEST?ns=registry.k8s.io | jq\n</code></pre> <p>Result: <pre><code>{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 2425,\n    \"digest\": \"sha256:406945b5115423a8c1d1e5cd53222ef2ff0ce9d279ed85badbc4793beebebc6c\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 84572,\n      \"digest\": \"sha256:aba5379b9c6dc7c095628fe6598183d680b134c7f99748649dddf07ff1422846\"\n    },\n    etc...\n</code></pre></p>"},{"location":"quickstart-desktop/#inspect-the-image-cache","title":"Inspect the image cache","text":"<pre><code>find /tmp/images\n</code></pre> <p>Result:</p> <pre><code>/tmp/images\n/tmp/images/blobs\n/tmp/images/blobs/fcb6f6d2c9986d9cd6a2ea3cc2936e5fc613e09f1af9042329011e43057f3265\n/tmp/images/blobs/e5dbef90bae3c9df1dfd4ae7048c56226f6209d538c91f987aff4f54e888f566\n/tmp/images/blobs/e8c73c638ae9ec5ad70c49df7e484040d889cca6b4a9af056579c3d058ea93f0\netc..\n/tmp/images/img\n/tmp/images/img/019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\n/tmp/images/img/a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\n</code></pre> <p>The manifest list was saved in: <code>images/img/a4afe5bf0ee...</code> and the image manifest was saved in: <code>images/img/019d7877d1...</code>.</p> <p>When you curled the image manifest the server pulled and cached the blobs at the same time and stored them in <code>images/blobs</code></p>"},{"location":"quickstart-desktop/#restart-the-server-and-repeat","title":"Restart the Server and Repeat","text":"<p>Stop the server with CTRL-C. Re-run, this time run with <code>info</code> logging for more visibility into what the server is doing. (The default logging level is <code>error</code>.)</p> <pre><code>bin/ociregistry --image-path /tmp/images --log-level info serve --port 8080\n</code></pre> <p>Run the same two curl commands.</p>"},{"location":"quickstart-desktop/#observe-the-logs","title":"Observe the logs","text":"<p>You will notice that the manifest list and the image manifest are now being returned from cache. You can see this in the logs:</p> <pre><code>INFO[0000] server is running                            \nINFO[0007] serving manifest from cache: \"registry.k8s.io/kube-scheduler:v1.29.1\" \nINFO[0007] echo server GET:/v2/kube-scheduler/manifests/v1.29.1?ns=registry.k8s.io status=200 latency=663.938\u00b5s host=localhost:8888 ip=::1 \nINFO[0010] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\" \nINFO[0010] echo server GET:/v2/kube-scheduler/manifests/sha256:019d7877d1?ns=registry.k8s.io status=200 latency=949.646\u00b5s host=localhost:8888 ip=::1 \n</code></pre>"},{"location":"quickstart-desktop/#docker-pull-through-the-server","title":"Docker pull (through the server)","text":"<p>If you have Docker (or Podman, or Crane, or your other favorite registry client), you can pull the image through the Ociregistry server. This uses the in-path image url form that both Docker and Ociregistry understand:</p> <pre><code>docker pull localhost:8080/registry.k8s.io/kube-scheduler:v1.29.1\n</code></pre> <p>Result:</p> <pre><code>v1.29.1: Pulling from registry.k8s.io/kube-scheduler\naba5379b9c6d: Pull complete \ne5dbef90bae3: Pull complete \nfbe9343cb4af: Pull complete \nfcb6f6d2c998: Pull complete \ne8c73c638ae9: Pull complete \n1e3d9b7d1452: Pull complete \n4aa0ea1413d3: Pull complete \n65efb1cabba4: Pull complete \n13547472c521: Pull complete \n53f492e4d27a: Pull complete \n6523efc24f16: Pull complete \nDigest: sha256:a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\nStatus: Downloaded newer image for localhost:8888/registry.k8s.io/kube-scheduler:v1.29.1\nlocalhost:8888/registry.k8s.io/kube-scheduler:v1.29.1\n</code></pre>"},{"location":"quickstart-desktop/#observe-the-new-log-entries","title":"Observe the new log entries","text":"<p>The Ociregistry server displays new log entries that show the image is being served from cache:</p> <pre><code>...\nINFO[0294] get /v2/                                     \nINFO[0294] echo server GET:/v2/ status=200 latency=117.389\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler:v1.29.1\" \nINFO[0294] echo server HEAD:/v2/registry.k8s.io/kube-scheduler/manifests/v1.29.1 status=200 latency=353.63\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\" \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/manifests/sha256:a4afe5bf0e status=200 latency=341.107\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\" \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/manifests/sha256:019d7877d1 status=200 latency=471.765\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:406945b511 status=200 latency=458.581\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:aba5379b9c status=200 latency=1.187488ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:fbe9343cb4 status=200 latency=1.730496ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:e5dbef90ba status=200 latency=1.452105ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:fcb6f6d2c9 status=200 latency=916.319\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:e8c73c638a status=200 latency=855.956\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:1e3d9b7d14 status=200 latency=836.118\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:4aa0ea1413 status=200 latency=654.022\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:65efb1cabb status=200 latency=581.97\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:13547472c5 status=200 latency=1.036117ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:53f492e4d2 status=200 latency=1.654236ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:6523efc24f status=200 latency=47.179364ms host=localhost:8888 ip=127.0.0.1 \n</code></pre>"},{"location":"quickstart-desktop/#observe-the-image-is-the-docker-cache","title":"Observe the image is the Docker cache","text":"<p>Running <code>docker image ls</code> should show the newly pulled image:</p> <pre><code>REPOSITORY                                      TAG       IMAGE ID       CREATED         SIZE\nlocalhost:8888/registry.k8s.io/kube-scheduler   v1.29.1   406945b51154   20 months ago   59.5MB\n</code></pre>"},{"location":"quickstart-desktop/#summary","title":"Summary","text":"<p>In this quick start you built the server on your desktop, ran it, and pulled an image through it. You verified that the first pull pulled through the Ociregistry server to the upstream, but a subsequent pull served the manifests and blobs from the Ociregistry server's cache.</p>"},{"location":"quickstart-kubernetes/","title":"Quick Start (Kubernetes in three steps)","text":"<p>The chart is hosted on Artifacthub. For the basic configuration you can install with no <code>values.yaml</code> file.</p>"},{"location":"quickstart-kubernetes/#1-install-the-chart","title":"1) Install the chart","text":"<pre><code>CHARTVER=n.n.n\nhelm upgrade --install ociregistry oci://quay.io/appzygy/helm-charts/ociregistry\\\n  --version $CHARTVER\\\n  --namespace ociregistry\\\n  --create-namespace\n</code></pre> <p>By default the chart will create a <code>NodePort</code> service on port <code>31080</code> in your cluster for <code>containerd</code> to mirror to. (This is configurable via a values  override.) If you recall, a NodePort service enables every node in the cluster to route traffic received on that node and port to the pod bound to that service, regardless of what node is running the pod. We will take advantage of this in the next step.</p>"},{"location":"quickstart-kubernetes/#2-configure-containerd","title":"2) Configure <code>containerd</code>","text":"<p>Configure containerd in your Kubernetes cluster to mirror all image pulls to the pull-through registry. (This has been tested with containerd &gt;= <code>v1.7.6</code>):</p> <p>First, add a <code>config_path</code> entry to <code>/etc/containerd/config.toml</code> to tell containerd to load all registry mirror configurations from that directory:</p> <pre><code>   ...\n   [plugins.\"io.containerd.grpc.v1.cri\".registry]\n      config_path = \"/etc/containerd/certs.d\"\n   ...\n</code></pre> <p>Then create a configuration file that tells containerd to pull from the caching pull-through registry server in the cluster. This is an example for <code>_default</code> which indicates to containerd that all images should be mirrored. Notice that the configuration simply uses <code>localhost</code> and the NodePort port value:</p> <pre><code>mkdir -p /etc/containerd/certs.d/_default &amp;&amp; \\\ncat &lt;&lt;EOF &gt;| /etc/containerd/certs.d/_default/hosts.toml\n[host.\"http://localhost:31080\"]\n  capabilities = [\"pull\", \"resolve\"]\n  skip_verify = true\nEOF\n</code></pre> <p>Key Points:</p> <ol> <li>The resolve capability tells containerd that a HEAD request to the server with a manifest will return a manifest digest. The pull capability indicates to containerd that the image can be pulled.</li> <li>Assuming you installed the caching pull-through OCI registry with the default <code>NodePort</code> service option on port <code>31080</code>, every host on the cluster will route <code>localhost:31080</code> to the Pod running the registry.</li> </ol> <p>The <code>containerd</code> daemon should detect the change and re-configure itself. If you believe that's not occurring, then <code>systemctl restart containerd</code>.</p>"},{"location":"quickstart-kubernetes/#3-verify","title":"3) Verify","text":""},{"location":"quickstart-kubernetes/#tail-the-logs-on-the-ociregistry-pod","title":"Tail the logs on the Ociregistry pod:","text":"<pre><code>kubectl -n ociregistry logs -f -l app.kubernetes.io/name=ociregistry\n</code></pre>"},{"location":"quickstart-kubernetes/#observe-the-startup-logs","title":"Observe the startup logs","text":"<pre><code>time=\"2025-04-24T21:11:41Z\" level=info msg=\"loaded 0 manifest(s) from the file system in 102.136\u00b5s\"\n----------------------------------------------------------------------\nOCI Registry: pull-only, pull-through, caching OCI Distribution Server\nVersion: vZ.Z.Z, build date: 2025-04-23T00:35:28.79Z\nStarted: 2025-04-24 21:11:41.727569108 +0000 UTC (port 8080)\nRunning as (uid:gid) 65532:65532\nProcess id: 1\nTls: none\nCommand line: /ociregistry/server --config-file /var/ociregistry/config/registry-config.yaml serve\n----------------------------------------------------------------------\ntime=\"2025-04-24T21:11:41Z\" level=info msg=\"server is running\"\n</code></pre>"},{"location":"quickstart-kubernetes/#run-the-hello-world-container","title":"Run the <code>hello-world</code> container","text":"<pre><code>kubectl run hello-world --image docker.io/hello-world:latest &amp;&amp;\\\n  sleep 5s &amp;&amp;\\\n  kubectl logs hello-world\n</code></pre>"},{"location":"quickstart-kubernetes/#result","title":"Result","text":"<p><pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n</code></pre> (remainder redacted for brevity...)</p>"},{"location":"quickstart-kubernetes/#observe-the-new-ociregistry-log-entries","title":"Observe the new Ociregistry log entries","text":"<pre><code>time=\"2025-04-24T21:13:54Z\" level=info msg=\"pulling manifest from upstream: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=501.131286ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world@sha256:c41088499908a59aae84b0a49c70e86f4731e588a737f1637e73c8c09d995654\\\"\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"echo server GET:/v2/library/hello-world/manifests/sha256:c410884999?ns=docker.io status=200 latency=2.267101ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"pulling manifest from upstream: \\\"docker.io/library/hello-world@sha256:03b62250a3cb1abd125271d393fc08bf0cc713391eda6b57c02d1ef85efcc25c\\\"\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/manifests/sha256:03b62250a3?ns=docker.io status=200 latency=612.467708ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/blobs/sha256:74cc54e27d?ns=docker.io status=200 latency=234.948\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/blobs/sha256:e6590344b1?ns=docker.io status=200 latency=232.113\u00b5s host=localhost:31080 ip=10.200.0.232\"\n</code></pre>"},{"location":"quickstart-kubernetes/#run-the-hello-world-container-again-with-a-different-name","title":"Run the <code>hello-world</code> container again with a different name","text":"<p>This time, also specify a pull policy to force <code>containerd</code> to pull the image:</p> <pre><code>kubectl run hello-world-2 --image docker.io/hello-world:latest\\\n  --image-pull-policy=Always\n</code></pre>"},{"location":"quickstart-kubernetes/#observe-new-ociregistry-log-entries","title":"Observe new Ociregistry log entries","text":"<p>These entries indicate that the Ociregistry server is serving from the image cache instead of re-pulling from DockerHub:</p> <pre><code>time=\"2025-04-24T21:34:35Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:35Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=204.413\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:34:36Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:36Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=358.099\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:34:51Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:51Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=313.859\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:35:09Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:35:09Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=836.294\u00b5s host=localhost:31080 ip=10.200.0.232\"\n</code></pre>"},{"location":"quickstart-kubernetes/#summary","title":"Summary","text":"<p>In this quick start, you Helm-installed Ociregistry in the cluster bound to a NodePort. You configured all containerd instances in the cluster to mirror to the in-cluster registry. You verified that an initial pull causes the Ociregistry server to pull through to the upstream (Docker Hub in this case) and that subsequent image pulls by containerd were served by the Ociregistry server from its cache, avoiding any subsequent round trips to the upstream.</p>"},{"location":"rest-api/","title":"Administrative REST API","text":"<p>The following REST endpoints are supported for administration of the image cache. Note - the output of the commands in some cases is columnar. Pipe through <code>column -t</code> to columnize.</p>"},{"location":"rest-api/#cmdprune","title":"<code>/cmd/prune</code>","text":"<p>Prunes the in-memory cache and the file system while the server is running.</p> Query param Description <code>type</code> Valid values: <code>accessed</code>, <code>created</code>, <code>pattern</code>. <code>dur</code> A duration string. E.g.: <code>30d</code>. Valid time units are <code>d</code>=days, <code>m</code>=minutes, and <code>h</code>=hours.  If <code>type</code> is <code>accessed</code>, then images that have not been accessed within the duration are pruned. If <code>type</code> is <code>created</code>, then images created earlier than the duration ago are pruned. (I.e.: created more than 30 days ago.) If <code>type</code> is <code>pattern</code>, then <code>dur</code> is ignored. <code>expr</code> If <code>type</code> is <code>pattern</code>, then a manifest URL pattern like <code>calico</code>, else ignored. Multiple patterns can be separated by commas: <code>foo,bar</code> <code>count</code> Max manifests to prune. Defaults to <code>50</code>. <code>-1</code> means no limit. <code>dryRun</code> If <code>true</code> then logs messages but does not prune. Defaults to false, meaning: will prune by default. <p>Example:</p> <pre><code>curl -X DELETE \"http://hostname:8080/cmd/prune?type=created&amp;dur=10d&amp;count=50&amp;dryRun=true\"\n</code></pre> <p>Explanation: Prunes manifests created (initially downloaded) more than 10 days ago. Only prune a max of 50. Since dry run is true, doesn't actually prune - only show what prune would do.</p>"},{"location":"rest-api/#cmdimagelist","title":"<code>/cmd/image/list</code>","text":"<p>Lists image manifests, and the blobs that are referenced by the selected manifests.</p> Query param Description <code>pattern</code> Comma-separated go regex expressions of manifest URL(s). <code>digest</code> Digest (or substring) of any blob referenced by the image. (Not the manifest digest!) <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/image/list?pattern=docker.io&amp;count=10\"\n</code></pre></p> <p>Explanation: List a max of 10 image manifests with <code>docker.io</code> in the URL.</p>"},{"location":"rest-api/#cmdbloblist","title":"<code>/cmd/blob/list</code>","text":"<p>Lists blobs and ref counts.</p> Query param Description <code>substr</code> Digest (or substring) of a blob. <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/blob/list?substr=56aebe9b&amp;count=10\"\n</code></pre></p>"},{"location":"rest-api/#cmdmanifestlist","title":"<code>/cmd/manifest/list</code>","text":"<p>List manifests.</p> Query param Description <code>pattern</code> Comma-separated go regex expressions of manifest URLs. <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/manifest/list?pattern=calico,cilium&amp;count=10\"\n</code></pre></p>"},{"location":"rest-api/#cmdstop","title":"<code>/cmd/stop</code>","text":"<p>Stops the server.</p>"},{"location":"systemd/","title":"Running Ociregistry as a systemd Service","text":"<p>The <code>systemd-service</code> directory has a systemd unit file for running the server as a systemd service:</p> <pre><code>[Unit]\nDescription=ociregistry\nDocumentation=https://github.com/aceeric/ociregistry\nAfter=network.target\n\n[Service]\nExecStart=/bin/ociregistry-server\\\n  --image-path=/var/lib/ociregistry\\\n  --log-level=info\\\n  serve\nType=simple\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>You can use the provided <code>manual-install</code> script in that directory to perform the installation.</p>"},{"location":"test-driver/","title":"Test Driver","text":"<p>The test driver CLI supports load testing. It performs two types of tests depending on options provided on the command line:</p> <ol> <li>Pulling through an Ociregistry server from an upstream registry</li> <li>Pulling only cached images from an Ociregistry server </li> </ol> <p>The CLI supports gradually scaling up and then scaling down the number of concurrent goroutines pulling from the server, and tallies the pull rate over time logging either to stdout or to a file.</p> <p>Running with no args will display help. The following options are supported. Details are provided below the summary table:</p> Arg Description Other info <code>--prune</code> Enables pruning. Boolean - default false <code>--iteration-seconds VALUE</code> Seconds that a goroutine will run before the next goroutine is started (scale up) or stopped (scale down) Default: 60 <code>--tally-seconds VALUE</code> Interval for tallying pull rate Default: 15 <code>--patterns VALUE</code> Comma-separated batching go regex patterns Can specify multiple. Aat least one is required. <code>*</code> is a valid pattern. <code>--metrics-file VALUE</code> Path to metrics output file <code>stdout</code> if omitted <code>--log-file VALUE</code> Path to log file <code>stdout</code> if omitted <code>--registry-url VALUE</code> Upstream registry URL (what Ociregistry server is pulling from) Required <code>--pullthrough-url VALUE</code> Pull through URL (Ociregistry server under test) Required <code>--filter VALUE</code> Repo filter Optional go regex expression to create a smaller test set from the upstream registry. <code>--dry-run</code> Does everything except actually pull from the registry Boolean - default false <code>--shuffle</code> If specified, then shuffles the image list between pull passes Boolean - default false <p>Arguments can be specified in two ways: <code>--arg=value</code> or <code>--arg value</code>.</p>"},{"location":"test-driver/#example","title":"Example","text":"<p>In this example, the ociregistry server under test is available on <code>ubuntu.me:8080</code>, and it is pulling from an \"upstream\" registry available on <code>ubuntu.me:5000</code>: <pre><code>go run .\\\n  --pullthrough-url=ubuntu.me:8080\\\n  --registry-url=ubuntu.me:5000\\\n  --patterns=-0001,-0002,-0003,-0004,-0005\\\n  --iteration-seconds=60\\\n  --tally-seconds=5\\\n  --dry-run\n</code></pre></p>"},{"location":"test-driver/#detailed-arg-documentation","title":"Detailed arg documentation","text":"Arg Description Prune If <code>true</code> then each goroutine will prune the images it pulled through the Ociregistry server on each pass before starting the next pass. This essentially forces the Ociregistry server under test to always be going to the upstream registry. Iteration Seconds This is the duration that the test driver will run each puller goroutine before the next goroutine is scaled up or down. Therefore, a test with 5 patterns, and 60 iteration seconds would take ten minutes: 5 minutes to scale up, and 5 minutes to scale down. Tally Seconds This is the sample interval used to determine the pull rate across all puller goroutines. Its the same idea as a prometheus scrape interval. Every this number of seconds the test driver will emanate the total pull rate across all puller goroutines. Patterns The patterns define the parallelism. For example, assume the upstream registry being pulled from has 1000 images and each image name contains a component like <code>-0001</code>, <code>-0002</code>, etc. E.g. <code>testimage-&lt;random number&gt;-0001:&lt;tag&gt;</code>, <code>testimage-&lt;random number&gt;-0002:&lt;tag&gt;</code>, etc. These patterns are specified on the command line as a comma-separated string. The test driver will start one goroutine to pull the <code>-0001</code> images. Then the second goroutine would pull the <code>-0002</code> images, and so on. So 10 patterns will result in 10 concurrently running goroutines each pulling their own filter. Note that <code>*,*,*,*,*</code> is a valid value for this arg. Metrics File The pull rate metrics are written to this file if specified, otherwise written to <code>stdout</code>. Log File Log messages are written to this file if specified, otherwise written to <code>stdout</code>. Registry url This is the \"upstream\" registry the Ociregistry server will pull from. It must be listening on HTTP. For example, I run the <code>registry</code> image in a container on port 5000, so: <code>registry-url=ubuntu.me:5000</code> Pull through url This is the url of the Ociregistry server under test. Because the test driver currently only supports HTTP, Ociregistry must be listening on HTTP - and - you will have to configure the Ociregistry under test to pull from the upstream Registry (below) on HTTP. See the configuration exemplar below this table. Since Ociregistry runs on 8080 by default, then: <code>--pullthrough-url=ubuntu.me:8080</code> Filter Allows to filter the images in the upstream registry (that Ociregistry is pulling from) before the test starts. Say your upstream registry has 1000 images and you know that the tags allow you to get a subset of that. You specify that filter here to get a smaller starting set. Otherwise, all 1000 images will be used for the test. Dry Run Does everything except pull through the Ociregistry server under test. (Instead, sleeps for a few milliseconds so as not to peg the CPU.) Also skips pruning. Shuffle If <code>true</code> then each goroutine will shuffle the image list on each iteration."},{"location":"test-driver/#configuring-ociregistry-for-test","title":"Configuring Ociregistry for Test","text":"<p>The test driver only supports HTTP at this time. The test driver gets its test set directly from the upstream registry that Ociregistry is pulling from. After getting the test set, then the test driver pulls through Ociregistry which pulls from the upstream. So since the upstream has to serve on HTTP, you have to configure Ociregistry to pull from the upstream over HTTP. The configuration example below accomplishes this via the <code>scheme</code> key: <pre><code>cat &lt;&lt;EOF &gt; /tmp/test-config.yaml\nregistries:\n  - name: ubuntu.me:5000\n    description: The docker registry with the test images for Ociregistry to pull from\n    scheme: http\nEOF\n</code></pre></p> <p>Then: <code>ociregistry --config-file /tmp/test-config.yaml serve</code></p> <p>By default, Ociregistry serves on HTTP so if run that way then it is automatically reachable by the test driver.</p>"}]}