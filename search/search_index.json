{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#ociregistry","title":"Ociregistry","text":"<p>Ociregistry is a pull-only, pull-through, caching OCI Distribution server. That means:</p> <ol> <li>It exclusively provides pull capability. It does this by implementing a subset of the OCI Distribution Spec.</li> <li>It provides caching pull-through capability to multiple upstream registries: internal, air-gapped, or public; supporting the following types of access: anonymous, basic auth, HTTP, HTTPS (secure &amp; insecure), one-way TLS, and mTLS. In other words, one running instance of this server can simultaneously pull from <code>docker.io</code>, <code>quay.io</code>, <code>registry.k8s.io</code>, <code>ghcr.io</code>, your air-gapped registries, in-house corporate mirrors, etc.</li> </ol>"},{"location":"#goals","title":"Goals","text":"<p>The goal of the project is to build a performant, simple, reliable edge OCI Distribution server for Kubernetes. One of the overriding goals was simplicity: only one binary is needed to run the server, and all state is persisted as simple files on the file system under one subdirectory. This supports the following use cases for running Kubernetes:</p> <ol> <li>Edge clusters.</li> <li>Air-gapped clusters - loading the server in a connected environment, and then serving a cluster in an air gap.</li> <li>Offloading corporate OCI Registries to minimize load / dependency on corporate resources.</li> <li>Running the registry in-cluster as a Kubernetes workload, and then mirroring containerd to the registry within the cluster or across clusters.</li> <li>Supporting development environments. For example, I do a lot of Kubernetes experimentation at home. In my home environment I run small multi-VM Kubernetes clusters on my desktop. I run Ociregistry on my desktop as a systemd service and mirror <code>containerd</code> in my dev Kubernetes clusters to the systemd service.</li> <li>Avoiding rate-limiting, but also being a good internet citizen by lightening the load on the large (free) distribution servers provided to us by DockerHub and many others.</li> </ol> <p>Other distribution servers index for availability and fault tolerance at the cost of increased complexity. This is not a criticism - it is simply a fact of design trade-offs. This server indexes for simplicity and accepts the availability and fault tolerance provided by the host.</p>"},{"location":"#summary","title":"Summary","text":"<p>The goals of the project are:</p> <ol> <li>Implement a narrow set of use cases, mostly around serving as a caching mirror for Kubernetes clusters.</li> <li>Be simple, performant, and reliable.</li> </ol>"},{"location":"airgap-considerations/","title":"Airgap Considerations","text":"<p>As stated in the Overview, one objective of the project is to support running Kubernetes in air-gapped and DDIL environments. To accomplish this, you will likely adopt one of two approaches:</p> <ol> <li>Stand the cluster up in a connected environment, pre-load the distribution server with all required images, then disconnect and ship the cluster to its edge location.</li> <li>Ship the cluster to its edge location and pre-load the distribution server there using stable comms. If comms are later degraded or lost then the required images remain cached.</li> </ol> <p>There are two general ways to run the distribution server in support of this objective. These are now discussed.</p>"},{"location":"airgap-considerations/#kubernetes-workload","title":"Kubernetes Workload","text":"<p>Using the helm chart described elsewhere in this guide you can install the server as a Kubernetes <code>Deployment</code>, and configure containerd on each Node to access the distribution server on a <code>NodePort</code>. In order to make this work, two things are needed:</p> <ol> <li>The cluster needs persistent storage that is redundant and reliable. This means that when you down the cluster, ship it, and start it up at the edge, the persistent storage has to come up with the image cache intact.</li> <li>You need a tarball of the distribution server container image available in the edge environment for the following reason: when you start the cluster for the first time, if containerd has been configured to pull from the distribution server, the distribution server image may not be in cache and so that Pod won't start. This is a classic deadlock scenario. Therefore your cluster startup procedure will be to load the distribution server image tarball into each cluster's containerd cache before starting the cluster. Now Kubernetes can start the distribution server workload which in turn can serve images from its cache. Ideally your Kubernetes distribution of choice will support the ability to pre-load containerd from an image tarball at a configurable file system location.</li> </ol>"},{"location":"airgap-considerations/#systemd-service","title":"<code>systemd</code> Service","text":"<p>You can run the distribution server as a <code>systemd</code> service on one of the cluster nodes. This avoids the deadlock scenario associated with running the distribution server as a Kubernetes workload because the service will come up when the cluster instances are started and will therefore immediately be available to serve cached images.</p> <p>The risk is that the image cache only exists on one cluster instance. If this instance is lost, then the cluster is down until comms are re-established. This can be mitigated by replicating the cache across multiple cluster nodes. There are multiple tools available to support this. For example Syncthing could be run on each node to ensure that each node has a full copy of the image cache. If the node running the distribution server is lost then the nodes can have their containerd configuration modified to point to any one of the other nodes, and the distribution server systemd service can be started on that node.</p>"},{"location":"command-line/","title":"The Command Line","text":"<p>The command line parser uses the urfave/cli parser. Running the server with no arguments shows the following sub-commands:</p> <pre><code>NAME:\n   ociregistry - a pull-only, pull-through, caching OCI distribution server\n\nUSAGE:\n   ociregistry [global options] [command [command options]]\n\nCOMMANDS:\n   serve    Runs the server\n   load     Loads the image cache\n   list     Lists the cache as it is on the file system\n   prune    Prunes the cache on the filesystem (server should not be running)\n   version  Displays the version\n   help, h  Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --log-level string    Sets the minimum value for logging: debug, warn, info, or error (default: \"error\")\n   --config-file string  A file to load configuration values from (cmdline overrides file settings)\n   --image-path string   The path for the image cache (default: \"/var/lib/ociregistry\")\n   --log-file string     log to the specified file rather than the console\n   --help, -h            show help\n</code></pre> <p>The simplest way to run the  server with all defaults is:</p> <pre><code>ociregistry serve\n</code></pre> <p>Each sub-command also supports help, as expected. E.g.: <code>ociregistry serve --help</code></p>"},{"location":"concur-design/","title":"Concurrency Design","text":"<p>The server maintains an in-memory cache of all manifests on the file system. Access to the manifest cache is synchronized via the in-memory replica because each pull updates the manifest pull date/time both on the file system and in memory. This supports the ability to prune the cache by pull recency.</p>"},{"location":"concur-design/#image-in-cache","title":"Image in cache","text":"<p>In the simple case, where a manifest is in cache, if multiple clients pull concurrently, the goroutines serving the clients are briefly synchronized to get the manifest from cache and update the pull timestamp. Each goroutine then exits the synchronization block and runs concurrently to serve the image. In the diagram, there is a blue goroutine and a red goroutine running concurrently. In the illustration, the blue goroutine is first into the synchronization block. (Time elapses in the rightward direction.)</p> <p></p> <p>In the example, the red goroutine will be the last one to update the image pull timestamp.</p>"},{"location":"concur-design/#image-not-in-cache","title":"Image not in cache","text":"<p>If the image is not in cache, it is a bit more complex. The complexity is introduced by the design goal to make efficient use of the network. So rather than having multiple concurrent pulls from the upstream, if multiple goroutines simultaneously request the same image that is not in cache, only one goroutine will actually perform the pull, and all other (concurrent) goroutines will wait for that image.</p> <p>Here is how that looks:</p> <p></p> <ol> <li>As in the previous example, both goroutines are run concurrently by a client such as containerd. As before, each goroutine is sychronized when querying the image cache. But in this example, the image is not in cache.</li> <li>The blue goroutine is first so when it exits the synchronization block it initiates a pull from the upstream.</li> <li>The red goroutine is second to pull the non-existing image so when it exits the synchronization block it is parked, waiting for the blue goroutine to finish pulling the image. So the amount of time for two (or ten, or more) clients to concurrently request an image that needs to be pulled is about the same for all the goroutines. The difference is - only the pulling goroutine will actually go to the upstream and utilize the network.</li> <li>When the blue goroutine finishes the pull, it adds the image to the cache, which is synchronized. When it exits the synchronization block it signals all goroutines waiting for this particular image - signified by the star in the diagram. This un-parks the red goroutine. The blue goroutine continues on to serve the image asynchronously.</li> <li>The red goroutine accesses the image from the cache, sychronized.</li> <li>The red goroutine exits the synchronization block and serves the image asynchronously.</li> </ol> <p>Pull synchronization (from upstreams) is by image tag, and is separate from the synchronization for images in cache. In other words, ten clients pulling hello-world:v3 because it is un-cached and ten other clients pulling hello-world:v2 that is un-cached and ten other clients accessing hello-world:v1 from cache are three disjoint synchronization contexts that don't effect each other.</p>"},{"location":"concur-design/#blob-synchronization","title":"Blob synchronization","text":"<p>The concurrency design for blobs is a little different. The reason is that there is no need to update pull timestamps on blobs to support image pruning since blobs are children of image manifests. When a manifest is pruned the manifest blobs can simply be pruned by reference. Whereas a manifest is updated each time it is pulled, a blob is never updated. Therefore blobs are synchronized with a Go RWMutex.</p> <p>This enables many concurrent pulls with very minimal sycnhronization overhead, but supports the infrequent addition of blobs resulting from new image pulls, and the removal of blobs as a result of image pruning. When blobs are added because of new pulls, or removed because of pruning, this will temporarily lock the blob cache. The server offers optimization techniques for this. For example, when configuring background pruning, you can configure Ociregistry to prune small sets with greater frequency.</p> <p>For blobs, only the digest is cached in memory. This is used to efficiently determine if the blob is cached. Whereas manifests are served from memory, blobs are served from the file system.</p>"},{"location":"configuring-the-server/","title":"Configuring The Server","text":"<p>The server will accept configuration on the command line, or via a configuration yaml file. Most of the configuration file settings map directly to the command line. The exceptions are the <code>pruneConfig</code>, <code>registries</code>, and <code>serverTlsConfig</code> entries which are only accepted in the configuration file at this time, being too complex to easily represent as command line args.</p> <p>As one would expect the following values provide configuration with the lowest priority on the bottom and the highest priority on the top:</p> <pre><code>block\n  columns 1\n  a[\"Command line\"]\n  b[\"Config file\"]\n  c[\"Hard-coded defaults in the Go code\"]</code></pre> <p>To provide full configuration in a file, run the server this way:</p> <pre><code>bin/ociregistry --config-file &lt;some file&gt; serve\n</code></pre> <p>But this is also valid:</p> <pre><code>bin/ociregistry --config-file &lt;some file&gt; serve --port 9999\n</code></pre>"},{"location":"configuring-the-server/#defaults","title":"Defaults","text":"<p>The yaml document below shows a <code>config.yaml</code> file specifying the defaults that are implemented in the Go code. In other words, running the server with this exact configuration file is the same as running with no configuration file:</p> <pre><code>imagePath: /var/lib/ociregistry\nlogLevel: error\nlogFile:\npreloadImages:\nimageFile:\nport: 8080\n#os: omit to use the server OS\n#arch: omit to use the server architecture\npullTimeout: 60000\nalwaysPullLatest: false\nairGapped: false\nhelloWorld: false\nhealth:\nregistries: []\npruneConfig:\n  enabled: false\nserverTlsConfig: {}\n</code></pre>"},{"location":"configuring-the-server/#config-file-keys-and-values","title":"Config file keys and values","text":"Key Type Default Command line arg Description <code>imagePath</code> Path spec /var/lib/ociregistry <code>--image-path</code> The base path for the image cache. The server will create sub-directories under this for blobs and manifests. <code>logLevel</code> keyword error <code>--log-level</code> Error level logging. See help for valid values. <code>logFile</code> Path spec - <code>--log-file</code> Empty means log to stderr. If you specify a file, the logging is directed to the file. <code>preloadImages</code> Path spec - <code>--preload-images</code> If the <code>serve</code> subcommand is specified, then this is the file containing a list of images to pre-load before starting the server. <code>imageFile</code> Path spec - <code>--image-file</code> If the <code>load</code> subcommand is specified, then this is the file containing a list of images to load. <code>port</code> Integer 8080 <code>--port</code> The port to serve on <code>os</code> keyword runtime.GOOS <code>--os</code> If loading or preloading, the OS and arch. If empty, then defaults to the host running the server. So usually comment these out. <code>arch</code> keyword runtime.GOARCH <code>--arch</code> \" <code>pullTimeout</code> Integer 60000 <code>--pull-timeout</code> Number of milliseconds before a pull from an upstream will time out. <code>alwaysPullLatest</code> Boolean false <code>--always-pull-latest</code> If true, then whenever a latest tag is pulled, the server will always pull from the upstream - in other words it acts like a basic proxy. Useful when supporting dev environments where latest is frequently changing. <code>airGapped</code> Boolean false <code>--air-gapped</code> If true, will not attempt to pull from an upstream when an image is requested that is not cached. <code>helloWorld</code> Boolean false <code>--hello-world</code> For testing. Only serves 'docker.io/hello-world:latest' from embedded blobs and manifests <code>health</code> Integer - <code>--health</code> A port number to run a /health endpoint on for Kubernetes liveness and readiness. By default, the server doesn't listen on a health port. The Helm chart enables this by default when running the server as a cluster workload. <code>registries</code> List of dictionary <code>[]</code> n/a Upstream registries configuration. See further down for registry configuration. <code>pruneConfig</code> Dictionary see below n/a Prune configuration. Pruning is disabled by default. See further down for prune configuration. <code>serverTlsConfig</code> Dictionary <code>{}</code> n/a Configures TLS with downstream (client) pullers, e.g. containerd. By default, serves over HTTP. See server tls configuration further down."},{"location":"configuring-the-server/#registry-configuration","title":"Registry Configuration","text":"<p>The OCI Distribution server may need configuration to connect to upstream registries. If run with no configuration for a given registry, the server will default to anonymous insecure <code>HTTPS</code> access. You specify registry configuration using the <code>registries</code> list:</p> <pre><code>registries:\n- name: upstream one\n  description: foo\n  scheme: https\n  auth: {}\n  tls: {}\n- name: upstream two\n  description: bar\n  scheme: http\n  auth: {}\n  tls: {}\n- etc...\n</code></pre> <p>Each entry supports the following configuration structure overall. Not all values are required. More detail is presented on that below. This shows the full structure:</p> <pre><code>- name: my-upstream # or my-upstream:PORT, e.g. index.docker.io\n  description: Something that makes sense to you (or omit it - it is optional)\n  scheme: https # (the default), also accepts http\n  auth:\n    user: theuser\n    password: thepass\n  tls:\n    ca: /my/ca.crt\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: true/false # defaults to false\n</code></pre> <p>Since <code>scheme</code> defaults to <code>https</code> you can omit that entirely. The <code>tls</code> key is optional. If omitted and <code>scheme</code> is https then the Ociregistry server attempts insecure 1-way TLS. The default for <code>tls.insecureSkipVerify</code> is <code>false</code> if omitted (and <code>tls</code> is specified.) Similarly, <code>description</code> is ignored by the server and can be omitted.</p> <p>The <code>auth</code> section implements basic auth, just like your <code>~/.docker/config.json</code> file.</p> <p>The <code>name</code> value is the upstream host name that you will pull from. For example say you're running the server on your desktop in test mode on <code>localhost:8080</code>. Let's also say that you have an in-house corporate registry that serves proprietary images on <code>my-corp-registry.myco.org:8888</code>. You run this command to pull through the Ociregistry server from your corporate registry server this way: <pre><code>docker pull localhost:8080/my-corp-registry.myco.org:8888/my-proprietary-app:v1.0.0\n</code></pre> Say your corp registry server requires user and password. Then you would configure a <code>registries</code> entry in the Ociregistry server config file like this: <pre><code>- name: my-corp-registry.myco.org:8888\n  auth:\n    user: theuser\n    password: thepass\n</code></pre></p> <p>Here's another scenario. Let's say your corp DNS resolves <code>index.docker.io</code> to an in-house registry mirror that requires an NPE cert. Then your registry entry might look like: <pre><code>- name: index.docker.io\n  tls:\n    cert: npe.crt\n    key: npe.key\n</code></pre></p> <p>The <code>tls</code> section can implement multiple scenarios:</p> <ol> <li>One-way insecure TLS, in which client certs are not provided to the remote, and the remote server cert is not validated:    <pre><code>tls:\n  insecureSkipVerify: true\n</code></pre></li> <li>One-way secure TLS, in which client certs are not provided to the remote, and the remote server cert is validated using the OS trust store:    <pre><code>tls:\n  insecureSkipVerify: false # (or omit, since it defaults to false)\n</code></pre></li> <li>One-way secure TLS, in which client certs are not provided to the remote, and the remote server cert is validate using a provided CA cert:    <pre><code>tls:\n  ca: /my/ca.crt\n</code></pre></li> <li>mTLS (client certs are provided to the remote):    <pre><code>tls:\n  cert: /my/client.cert\n  key: /my/client.key\n</code></pre>    mTLS can be implemented with and without remote server cert validation as described above in the various one-way TLS scenarios. Examples:    <pre><code>- name: foo.bar.1.io\n  description: mTLS, don't verify server cert\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: true\n- name: foo.bar.2.io\n  description: mTLS, verify server cert from OS trust store\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    insecureSkipVerify: false\n- name: foo.bar.3.io\n  description: mTLS, verify server cert from provided CA\n  tls:\n    cert: /my/client.cert\n    key: /my/client.key\n    ca: /remote/ca.crt\n    insecureSkipVerify: false\n</code></pre></li> </ol>"},{"location":"configuring-the-server/#server-tls-configuration","title":"Server TLS configuration","text":"<p>By default, the server serves over HTTP. The <code>serverTlsConfig</code> section in the config file configures the Ociregistry server to serve over TLS. </p> <p>The <code>cert</code>, <code>key</code>, and <code>ca</code> are expected to be PEM encoded files. Example:</p> <pre><code>serverTlsConfig:\n  cert: /path/to/pem/encoded/server.crt\n  key: /path/to/pem/encoded/server.private.key\n  ca: /path/to/pem/encoded/ca.crt\n  clientAuth: none # or verify\n</code></pre>"},{"location":"configuring-the-server/#server-tls-permutations","title":"Server TLS permutations","text":"<p>The following permutations are supported or serving over HTTPS:</p> Configuration Description <code>cert</code> and <code>key</code> populated The server will provide the cert to the client to establish 1-way TLS. <code>clientAuth: none</code> Client cert is not requested and will be ignored if provided. <code>clientAuth: verify</code> mTLS: Client cert is required and verified. See the <code>ca</code> key below. <code>ca</code> populated Client cert is validated against the provided CA. <code>ca</code> omitted Client cert is validated against the OS trust store."},{"location":"configuring-the-server/#prune-configuration","title":"Prune Configuration","text":"<p>Pruning configures the server to remove images as a background process based on create date or recency of a pull. (Each time an image is pulled the server updates the pull date/time for the image.) Pruning is disabled by default. An example full prune configuration is as follows:</p> <pre><code>pruneConfig:\n  enabled: true\n  duration: 30d\n  type: accessed\n  frequency: 1d\n  count: -1\n  dryRun: false\n</code></pre>"},{"location":"configuring-the-server/#prune-configuration-keys-and-values","title":"Prune configuration keys and values","text":"Key Type Description <code>enabled</code> Boolean If true, enables background pruning. <code>duration</code> Duration expression E.g.: <code>30d</code>. The value is interpreted based on the prune <code>type</code> below. Valid time units are <code>ns</code> (nanoseconds), <code>us</code> or <code>\u00b5s</code> (microseconds), <code>ms</code> (milliseconds), <code>s</code> (seconds), <code>m</code> (minutes), <code>h</code> (hours), and <code>d</code> (days). <code>type</code> Keyword Valid values are <code>accessed</code> and <code>created</code>. If <code>accessed</code>, then the server prunes images that have not been pulled in <code>duration</code> amount of time. If <code>created</code>, then the server prunes images whose create date is older than <code>duration</code> time ago. <code>frequency</code> Duration expression E.g.: <code>1d</code>. Run the background pruner with this frequency. The time units are the same as for <code>duration</code>. <code>count</code> Integer The number of images to prune on each run of the background pruner. A value of <code>-1</code> means no limit to the number of images pruned. <code>dryRun</code> Boolean If <code>true</code> then just log messages but don't actually prune. For testing and troubleshooting. <p>Since pruning locks the cache, a good strategy is to limit the number of pruned images on each invocation of the pruner and run with greater frequency.</p>"},{"location":"design/","title":"Design","text":"<p>This is the design of the Ociregistry server:</p> <p></p> <p>Narrative:</p> <ol> <li>A client (in this case - <code>containerd</code>) initiates an image pull. The image pull consists of a series of REST API calls. The API calls are handled by the server REST API, which implements a portion of the OCI Distribution Spec.</li> <li>The API is just a veneer that delegates to the server implementation.</li> <li>The server checks the local cache and if the image is in cache it is immediately returned from cache.</li> <li>If the image is not in cache, the server calls the embedded ImgPull library to pull the image from the upstream registry. The server knows which upstream to pull from because <code>containerd</code> appends a query parameter (e.g. <code>?ns=registry.k8s.io</code>) to each API call. <p>The server also supports in-path upstreams, e.g.: <code>docker pull ociregistry.host/registry.k8s.io/pause:3.8</code></p> </li> <li>The embedded image puller library pulls the image from the upstream registry and returns it to the server.</li> <li>The server adds the image to cache and returns the image to the caller from the newly updated cache.</li> </ol>"},{"location":"design/#image-pull-sequence-diagram","title":"Image Pull Sequence Diagram","text":"<p>By way of background, a typical image pull sequence is as follows. If you tail the logs of the Ociregistry server, this is what you'll see:</p> <pre><code>sequenceDiagram\n    Client-&gt;&gt;Server: HEAD the manifest list by tag, e.g. \"registry.k8s.io/pause:3.8\"\n    Server--&gt;&gt;Client: Digest of the manifest list in a response header (or 404 Not Found)\n    Client-&gt;&gt;Server: GET the manifest list by digest\n    Server--&gt;&gt;Client: Send a manifest list listing all available manifests\n    Client-&gt;&gt;Client: Pick an image manifest digest from the manifest list matching the desired OS and architecture\n    Client-&gt;&gt;Server: GET the image manifest by digest \n    Server--&gt;&gt;Client: Send the image manifest\n    Client-&gt;&gt;Server: GET the blobs for the image\n    Server--&gt;&gt;Client: Send the blobs</code></pre> <p>To support this, the server caches both the image list manifest and the image manifest.</p> <p>Be aware: If you cache an image list manifest by digest only and later run a workload in an air-gapped environment that attempts to get the same manifest by tag, the registry will not know the tag and so will not be able to provide that image from cache.</p>"},{"location":"design/#code-structure","title":"Code Structure","text":"<p>The source code is organized as shown:</p> <pre><code>project root\n\u251c\u2500\u2500 api\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 charts\n\u251c\u2500\u2500 cmd\n\u251c\u2500\u2500 docs\n\u251c\u2500\u2500 impl\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cache\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cmdline\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 globals\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 helpers\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 preload\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 pullrequest\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 serialize\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers.go\n\u2502   \u2514\u2500\u2500 ociregistry.go\n\u2514\u2500\u2500 mock\n</code></pre> Package Description <code>api</code> Mostly generated by <code>oapi-codegen</code> using the OAPI Spec <code>ociregistry.yaml</code> in that directory. <code>bin</code> Has the compiled server after <code>make server</code>. <code>charts</code> The Helm chart. <code>cmd</code> Entry point (<code>ociregistry.go</code>) and sub-commands. <code>docs</code> MKDocs documentation (Material theme.) <code>impl</code> Has the implementation of the server. <code>impl/cache</code> Implements the in-memory cache. <code>impl/cmdline</code> Parses the command line. <code>impl/config</code> Has system configuration. <code>impl/globals</code> Globals. <code>impl/helpers</code> Helpers. <code>impl/preload</code> Implements the load and pre-load from an image list file. <code>impl/pullrequest</code> Abstracts the URL parts of an image pull. <code>impl/serialize</code> Reads/writes from/to the file system. <code>impl/handlers.go</code> Has the code for the subset of the OCI Distribution Server API spec that the server implements. <code>impl/ociregistry.go</code> A veneer that the embedded Echo server calls that simply delegates to <code>impl/handlers.go</code>. See the next section - REST API Implementation for some details on the REST API. <code>mock</code> Runs a mock upstream OCI Distribution server used by the unit tests."},{"location":"design/#rest-api-implementation","title":"REST API Implementation","text":"<p>As stated above, the Ociregistry server implements a portion of the OCI Distribution Spec consisting of only the endpoints in the spec needed to meet its goal of being a pull-only OCI Distribution Server. It does this by running a web service that handles endpoints defined in the spec.</p> <p>The Ociregistry server REST API is built by first creating an Open API spec: see ociregistry.yaml in the <code>api</code> directory of the project. Then the oapi-codegen tool is used to generate the API code and the Model code using configuration in the <code>api</code> directory of the project. This approach was modeled after the OAPI-Codegen Petstore example.</p> <p>Oapi-codegen is installed by the following command:</p> <pre><code>go install github.com/oapi-codegen/oapi-codegen/v2/cmd/oapi-codegen@latest\n</code></pre> <p>The key components of the API scaffolding supported by OAPI-Codegen are shown below:</p> <pre><code>\u251c\u2500\u2500 api\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2502   \u2514\u2500\u2500models.gen.go   (generated)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models.cfg.yaml    (modeled from pet store)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ociregistry.gen.go (generated)\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 server.cfg.yaml    (modeled from pet store)\n\u251c\u2500\u2500 cmd\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ociregistry.go     (this is the server - which embeds the Echo server)\n\u2514\u2500\u2500 ociregistry.yaml       (the openapi spec built with swagger)\n</code></pre> <p>I elected to use the Echo option to run the API. The Echo server is started by the serve sub-command of the Ociregistry server.</p>"},{"location":"loading/","title":"Loading And Pre-Loading","text":"<p>Loading and Pre-loading supports the air-gapped use case of populating the registry in a connected environment, and then moving it into an air-gapped environment.</p> <p>You can pre-load the cache two ways:</p> <ol> <li>As a startup task before running the service: <code>bin/ociregistry serve --preload-images &lt;file&gt;</code>. The server will load the image cache and then serve.</li> <li>By using the binary as a CLI: <code>bin/ociregistry load --image-file &lt;file&gt;</code>. The executable will load the cache and then exit back to the command prompt.</li> </ol> <p>In both cases, you create a file with a list of image references. Example:</p> <pre><code>cat &lt;&lt;EOF &gt;| imagelist\nquay.io/jetstack/cert-manager-cainjector:v1.11.2\nquay.io/jetstack/cert-manager-controller:v1.11.2\nquay.io/jetstack/cert-manager-webhook:v1.11.2\nregistry.k8s.io/metrics-server/metrics-server:v0.6.2\nregistry.k8s.io/ingress-nginx/controller:v1.8.1\nregistry.k8s.io/pause:3.8\ndocker.io/kubernetesui/dashboard-api:v1.0.0\ndocker.io/kubernetesui/metrics-scraper:v1.0.9\ndocker.io/kubernetesui/dashboard-web:v1.0.0\nEOF\n</code></pre> <p>Since the entirety of the image cache consists of files and sub-directories under the image cache directory, you can tar that directory up at any time, copy it somewhere, untar it, and start an Ociregistry server instance there pointing to the copied directory and it will just work.</p>"},{"location":"loading/#image-store","title":"Image Store","text":"<p>The image store is persisted to the file system. This includes blobs and manifests. Let's say you run the server with <code>--image-path=/var/lib/ociregistry</code>, which is the default. Then:</p> <pre><code>/var/lib/ociregistry\n\u251c\u2500\u2500 blobs\n\u251c\u2500\u2500 img\n\u2514\u2500\u2500 lts\n</code></pre> <ol> <li><code>blobs</code> are where the blobs are stored.</li> <li><code>img</code> stores the non-<code>latest</code>-tagged image manifests.</li> <li><code>lts</code> stores the <code>latest</code>-tagged image manifests. (See About \"Latest\" below.)</li> </ol> <p>Everything is stored by digest. When the server starts it loads everything into an in-memory representation. Each new pull through the server while it is running updates both the in-memory representation of the image store as well as the persistent state on the file system.</p> <p>The software uses a data structure called a ManifestHolder to hold all the image metadata and the actual manifest bytes from the upstream registry. These are simply serialized to the file system as JSON. (So you can find and inspect them if needed for troubleshooting with <code>grep</code>, <code>cat</code>, and <code>jq</code>.)</p> <p>A <code>ManifestHolder</code> looks like this: <pre><code>type ManifestHolder struct {\n    Type                 ManifestType\n    Digest               string\n    ImageUrl             string\n    Bytes                []byte\n    V1ociIndex           v1oci.Index\n    V1ociManifest        v1oci.Manifest\n    V2dockerManifestList v2docker.ManifestList\n    V2dockerManifest     v2docker.Manifest\n    Created              string\n    Pulled               string\n}\n</code></pre></p> <p>The <code>Bytes</code> field has the actual manifest bytes from the upstream. You can see the supported manifest types: <code>V1ociIndex</code>, <code>V1ociManifest</code>, <code>V2dockerManifestList</code>, and <code>V2dockerManifest</code>.</p>"},{"location":"loading/#loading-behavior","title":"Loading behavior","text":"<p>Loading is additive, meaning if you run the load command to load 100 images, then run it again to load 100 different images, your image cache will have 200 images. If you load 100 images, and then later load the same 100 images again, the server will detect during the second load that there is nothing to do. And of course, if you first load A, B, and C, and then later load C, D, and E, then the cache will hold A, B, C, and D.</p>"},{"location":"loading/#about-latest","title":"About \"Latest\"","text":"<p>Internally, <code>latest</code>-tagged images are stored side-by-side with non-latest images and treated as separate manifests. This enables the server to support cases that occur in development environments where <code>latest</code> images are in a constant state of flux. Storing <code>latest</code> images this way works in tandem with the <code>--always-pull-latest</code> flag as follows:</p> Action <code>--always-pull-latest</code> Result Pull <code>foo:latest</code> <code>false</code> (the default) The image is pulled exactly once. All subsequent pulls return the same image regardless of what happens in the upstream. Pull <code>foo:latest</code> <code>true</code> The image is pulled from the upstream on each pull from the pull-through server for each client. Each pull completely replaces the prior pull. In other words - for latest images the server is a stateless proxy. (This could consume a fair bit of network bandwidth.)"},{"location":"pruning/","title":"Pruning The Image Cache","text":"<p>The compiled server binary can be used as a CLI to prune the cache on the file system.</p> <p>The server can also be configured to prune as a continual background process. For information on that, see Prune Configuration in the Configuring the Server page. There is also a REST API for ad-hoc pruning the running server's cache. See the Administrative REST API document for information on that.</p> <p>This page is about ad-hoc manual pruning using the server binary as a CLI.</p> <p>Important</p> <p>The server must be stopped while pruning using the binary as a CLI because the CLI only manipulates the file system, not the in-memory representation of the cache. Pruning removes manifest lists, manifests, and possibly blobs (more on blobs below.)</p>"},{"location":"pruning/#list-images","title":"List Images","text":"<p>Generally, it is expected that you will use the server binary as a CLI to list the images before deciding which images to prune. E.g.:</p> <pre><code>bin/ociregistry --image-path /my/image/cache list --pattern dashboard\n</code></pre> <p>Result (for example) truncated in the doc for readability:</p> <pre><code>docker.io/kubernetesui/dashboard-web@sha256:05ad8120...\ndocker.io/kubernetesui/dashboard-metrics-scraper@sha256:0cdefa04...\ndocker.io/kubernetesui/dashboard:v2.7.0\ndocker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\ndocker.io/kubernetesui/dashboard-api:1.12.0\ndocker.io/kubernetesui/dashboard-web:1.6.2\ndocker.io/kubernetesui/dashboard-auth:1.2.4\ndocker.io/kubernetesui/dashboard@sha256:ca93706e...\ndocker.io/kubernetesui/dashboard-auth@sha256:d6dd67b7...\ndocker.io/kubernetesui/dashboard-api@sha256:dcc897f8...\n</code></pre>"},{"location":"pruning/#dry-run","title":"Dry Run","text":"<p>It is strongly recommended to use the <code>--dry-run</code> arg to develop your pruning expression. Then remove <code>--dry-run</code> to actually prune the cache. When <code>--dry-run</code> is specified, the CLI shows you exactly what will be pruned but does not actually modify the file system.</p>"},{"location":"pruning/#by-pattern","title":"By Pattern","text":"<p>Specify <code>--pattern</code> with single parameter consisting of one or more manifest URL patterns separated by commas. The patterns are Golang regular expressions as documented in the regexp/syntax package documentation. The expressions on the command line are passed directly to the Golang <code>regex</code> parser as received from the shell, and are matched to manifest URLs. As such, shell expansion and escaping must be taken into consideration. Simplicity wins the day here. Examples:</p> <pre><code>bin/ociregistry prune --pattern kubernetesui/dashboard:v2.7.0 --dry-run\nbin/ociregistry prune --pattern docker.io --dry-run\nbin/ociregistry prune --pattern curl,cilium --dry-run\n</code></pre>"},{"location":"pruning/#by-create-datetime","title":"By Create Date/time","text":"<p>The <code>--prune-before</code> option accepts a single parameter consisting of a local date/time in the form <code>YYYY-MM-DDTHH:MM:SS</code>. All manifests created before that time stamp will be selected. Example:</p> <pre><code>bin/ociregistry prune --date 2024-03-01T22:17:31 --dry-run\n</code></pre> <p>The intended workflow is to use the CLI with <code>list</code> sub-command to determine desired a cutoff date and then to use that date as an input to the <code>prune</code> sub-command.</p>"},{"location":"pruning/#important-to-know-about-pruning","title":"Important to know about pruning","text":"<p>Generally, but not always, image list manifests have tags, and image manifests have digests. This is because in most cases, upstream images are multi-architecture. For example, this command specifies a tag:</p> <pre><code>bin/ociregistry prune --pattern calico/typha:v3.27.0 --dry-run\n</code></pre> <p>In this case, on a Linux/amd64 machine running the server the CLI will find two manifests:</p> <pre><code>docker.io/calico/typha:v3.27.0\ndocker.io/calico/typha@sha256:eca01eab...\n</code></pre> <p>The first is a multi-arch image list manifest, and the second is the image manifest matching the OS and Architecture that was selected for download. In all cases, only image manifests have blob references. If your search finds only an image list manifest, the CLI logic will also look for cached image manifests (and associated blobs) for the specified image list manifest since that's probably the desired behavior. (The blobs consume the storage.)</p>"},{"location":"pruning/#blob-removal-when-pruning","title":"Blob removal when pruning","text":"<p>Internally, the CLI begins by building a blob list with ref counts. As each image manifest is removed its referenced blobs have their count decremented. After all manifests are removed, any blob with zero refs is also removed. Removing an image manifest therefore won't remove blobs that are still referenced by un-pruned manifests.</p>"},{"location":"quickstart-desktop/","title":"Quick Start (Desktop)","text":"<p>The easiest way to understand how the server works is to git clone the repo, then build and run the server locally on your Linux desktop.</p> <p>After git-cloning the project:</p>"},{"location":"quickstart-desktop/#build-the-server","title":"Build the server","text":"<pre><code>make server\n</code></pre> <p>This command compiles the server and creates a binary called <code>ociregistry</code> in the <code>bin</code> directory relative to the project root.</p>"},{"location":"quickstart-desktop/#run-the-server","title":"Run the server","text":"<p>You provide a file system location for the image cache with the <code>--image-path</code> arg. If the directory doesn't exist the server will create it. The default is <code>/var/lib/ociregistry</code> but to kick the tires it makes more sense to use the system temp directory. By default the server listens on <code>8080</code>. If you have something already running and bound to that port, specify <code>--port</code>. We'll specify it explicitly here with the default value:</p> <pre><code>bin/ociregistry --image-path /tmp/images serve --port 8080\n</code></pre> <p>Result: <pre><code>----------------------------------------------------------------------\nOCI Registry: pull-only, pull-through, caching OCI Distribution Server\nVersion: 1.9.6, build date: 2025-10-06T23:35:46.56Z\nStarted: 2025-10-06 19:49:57.169081368 -0400 EDT (port 8080)\nRunning as (uid:gid) 1000:1000\nProcess id: 27010\nTls: none\nCommand line: bin/ociregistry --image-path /tmp/images serve --port 8080\n----------------------------------------------------------------------\n</code></pre></p>"},{"location":"quickstart-desktop/#curl-an-image-manifest-list","title":"Curl an image manifest list","text":"<p>Curl a manifest list using the OCI Distribution Server REST API. Note the <code>ns</code> query parameter in the URL below which tells the server to go to that upstream if the image isn't already locally cached. This is exactly how containerd does it when you configure containerd to mirror:</p> <pre><code>curl localhost:8080/v2/kube-scheduler/manifests/v1.29.1?ns=registry.k8s.io | jq\n</code></pre> <p>(The server also supports in-path namespaces like <code>localhost:8080/v2/registry.k8s.io/kube-scheduler/manifests/v1.29.1</code>)</p> <p>Result: <pre><code>  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n  \"manifests\": [\n    {\n      \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n      \"size\": 2612,\n      \"digest\": \"sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\",\n      \"platform\": {\n        \"architecture\": \"amd64\",\n        \"os\": \"linux\"\n      }\n    },\n    etc...\n</code></pre></p>"},{"location":"quickstart-desktop/#curl-an-image-manifest","title":"Curl an image manifest","text":"<p>Pick the first manifest from the list above - the <code>amd64/linux</code> manifest - and curl the manifest by SHA, again using the OCI Distribution Server REST API:</p> <pre><code>DIGEST=019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\ncurl localhost:8080/v2/kube-scheduler/manifests/sha256:$DIGEST?ns=registry.k8s.io | jq\n</code></pre> <p>Result: <pre><code>{\n  \"schemaVersion\": 2,\n  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n  \"config\": {\n    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n    \"size\": 2425,\n    \"digest\": \"sha256:406945b5115423a8c1d1e5cd53222ef2ff0ce9d279ed85badbc4793beebebc6c\"\n  },\n  \"layers\": [\n    {\n      \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\",\n      \"size\": 84572,\n      \"digest\": \"sha256:aba5379b9c6dc7c095628fe6598183d680b134c7f99748649dddf07ff1422846\"\n    },\n    etc...\n</code></pre></p>"},{"location":"quickstart-desktop/#inspect-the-image-cache","title":"Inspect the image cache","text":"<pre><code>find /tmp/images\n</code></pre> <p>Result:</p> <pre><code>/tmp/images\n/tmp/images/blobs\n/tmp/images/blobs/fcb6f6d2c9986d9cd6a2ea3cc2936e5fc613e09f1af9042329011e43057f3265\n/tmp/images/blobs/e5dbef90bae3c9df1dfd4ae7048c56226f6209d538c91f987aff4f54e888f566\n/tmp/images/blobs/e8c73c638ae9ec5ad70c49df7e484040d889cca6b4a9af056579c3d058ea93f0\netc..\n/tmp/images/img\n/tmp/images/img/019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\n/tmp/images/img/a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\n</code></pre> <p>The manifest list was saved in: <code>images/img/a4afe5bf0ee...</code> and the image manifest was saved in: <code>images/img/019d7877d1...</code>.</p> <p>When you curled the image manifest the server pulled and cached the blobs at the same time and stored them in <code>images/blobs</code></p>"},{"location":"quickstart-desktop/#restart-the-server-and-repeat","title":"Restart the Server and Repeat","text":"<p>Stop the server with CTRL-C. Re-run, this time run with <code>info</code> logging for more visibility into what the server is doing. (The default logging level is <code>error</code>.)</p> <pre><code>bin/ociregistry --image-path /tmp/images --log-level info serve --port 8080\n</code></pre> <p>Run the same two curl commands.</p>"},{"location":"quickstart-desktop/#observe-the-logs","title":"Observe the logs","text":"<p>You will notice that the manifest list and the image manifest are now being returned from cache. You can see this in the logs:</p> <pre><code>INFO[0000] server is running                            \nINFO[0007] serving manifest from cache: \"registry.k8s.io/kube-scheduler:v1.29.1\" \nINFO[0007] echo server GET:/v2/kube-scheduler/manifests/v1.29.1?ns=registry.k8s.io status=200 latency=663.938\u00b5s host=localhost:8888 ip=::1 \nINFO[0010] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\" \nINFO[0010] echo server GET:/v2/kube-scheduler/manifests/sha256:019d7877d1?ns=registry.k8s.io status=200 latency=949.646\u00b5s host=localhost:8888 ip=::1 \n</code></pre>"},{"location":"quickstart-desktop/#docker-pull-through-the-server","title":"Docker pull (through the server)","text":"<p>If you have Docker (or Podman, or Crane, or your other favorite registry client), you can pull the image through the Ociregistry server. This uses the in-path image url form that both Docker and Ociregistry understand:</p> <pre><code>docker pull localhost:8080/registry.k8s.io/kube-scheduler:v1.29.1\n</code></pre> <p>Result:</p> <pre><code>v1.29.1: Pulling from registry.k8s.io/kube-scheduler\naba5379b9c6d: Pull complete \ne5dbef90bae3: Pull complete \nfbe9343cb4af: Pull complete \nfcb6f6d2c998: Pull complete \ne8c73c638ae9: Pull complete \n1e3d9b7d1452: Pull complete \n4aa0ea1413d3: Pull complete \n65efb1cabba4: Pull complete \n13547472c521: Pull complete \n53f492e4d27a: Pull complete \n6523efc24f16: Pull complete \nDigest: sha256:a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\nStatus: Downloaded newer image for localhost:8888/registry.k8s.io/kube-scheduler:v1.29.1\nlocalhost:8888/registry.k8s.io/kube-scheduler:v1.29.1\n</code></pre>"},{"location":"quickstart-desktop/#observe-the-new-log-entries","title":"Observe the new log entries","text":"<p>The Ociregistry server displays new log entries that show the image is being served from cache:</p> <pre><code>...\nINFO[0294] get /v2/                                     \nINFO[0294] echo server GET:/v2/ status=200 latency=117.389\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler:v1.29.1\" \nINFO[0294] echo server HEAD:/v2/registry.k8s.io/kube-scheduler/manifests/v1.29.1 status=200 latency=353.63\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:a4afe5bf0eefa56aebe9b754cdcce26c88bebfa89cb12ca73808ba1d701189d7\" \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/manifests/sha256:a4afe5bf0e status=200 latency=341.107\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] serving manifest from cache: \"registry.k8s.io/kube-scheduler@sha256:019d7877d15b45951df939efcb941de9315e8381476814a6b6fdf34fc1bee24c\" \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/manifests/sha256:019d7877d1 status=200 latency=471.765\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:406945b511 status=200 latency=458.581\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:aba5379b9c status=200 latency=1.187488ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:fbe9343cb4 status=200 latency=1.730496ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:e5dbef90ba status=200 latency=1.452105ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:fcb6f6d2c9 status=200 latency=916.319\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:e8c73c638a status=200 latency=855.956\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:1e3d9b7d14 status=200 latency=836.118\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:4aa0ea1413 status=200 latency=654.022\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:65efb1cabb status=200 latency=581.97\u00b5s host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:13547472c5 status=200 latency=1.036117ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:53f492e4d2 status=200 latency=1.654236ms host=localhost:8888 ip=127.0.0.1 \nINFO[0294] echo server GET:/v2/registry.k8s.io/kube-scheduler/blobs/sha256:6523efc24f status=200 latency=47.179364ms host=localhost:8888 ip=127.0.0.1 \n</code></pre>"},{"location":"quickstart-desktop/#observe-the-image-is-the-docker-cache","title":"Observe the image is the Docker cache","text":"<p>Running <code>docker image ls</code> should show the newly pulled image:</p> <pre><code>REPOSITORY                                      TAG       IMAGE ID       CREATED         SIZE\nlocalhost:8888/registry.k8s.io/kube-scheduler   v1.29.1   406945b51154   20 months ago   59.5MB\n</code></pre>"},{"location":"quickstart-desktop/#summary","title":"Summary","text":"<p>In this quick start you built the server on your desktop, ran it, and pulled an image through it. You verified that the first pull pulled through the Ociregistry server to the upstream, but a subsequent pull served the manifests and blobs from the Ociregistry server's cache.</p>"},{"location":"quickstart-kubernetes/","title":"Quick Start (Kubernetes in three steps)","text":"<p>The chart is hosted on Artifacthub. For the basic configuration you can install with no <code>values.yaml</code> file.</p>"},{"location":"quickstart-kubernetes/#1-install-the-chart","title":"1) Install the chart","text":"<pre><code>CHARTVER=n.n.n\nhelm upgrade --install ociregistry oci://quay.io/appzygy/helm-charts/ociregistry\\\n  --version $CHARTVER\\\n  --namespace ociregistry\\\n  --create-namespace\n</code></pre> <p>By default the chart will create a <code>NodePort</code> service on port <code>31080</code> in your cluster for <code>containerd</code> to mirror to. (This is configurable via a values  override.) If you recall, a NodePort service enables every node in the cluster to route traffic received on that node and port to the pod bound to that service, regardless of what node is running the pod. We will take advantage of this in the next step.</p>"},{"location":"quickstart-kubernetes/#2-configure-containerd","title":"2) Configure <code>containerd</code>","text":"<p>Configure containerd in your Kubernetes cluster to mirror all image pulls to the pull-through registry. (This has been tested with containerd &gt;= <code>v1.7.6</code>):</p> <p>First, add a <code>config_path</code> entry to <code>/etc/containerd/config.toml</code> to tell containerd to load all registry mirror configurations from that directory:</p> <pre><code>   ...\n   [plugins.\"io.containerd.grpc.v1.cri\".registry]\n      config_path = \"/etc/containerd/certs.d\"\n   ...\n</code></pre> <p>Then create a configuration file that tells containerd to pull from the caching pull-through registry server in the cluster. This is an example for <code>_default</code> which indicates to containerd that all images should be mirrored. Notice that the configuration simply uses <code>localhost</code> and the NodePort port value:</p> <pre><code>mkdir -p /etc/containerd/certs.d/_default &amp;&amp; \\\ncat &lt;&lt;EOF &gt;| /etc/containerd/certs.d/_default/hosts.toml\n[host.\"http://localhost:31080\"]\n  capabilities = [\"pull\", \"resolve\"]\n  skip_verify = true\nEOF\n</code></pre> <p>Key Points:</p> <ol> <li>The resolve capability tells containerd that a HEAD request to the server with a manifest will return a manifest digest. The pull capability indicates to containerd that the image can be pulled.</li> <li>Assuming you installed the caching pull-through OCI registry with the default <code>NodePort</code> service option on port <code>31080</code>, every host on the cluster will route <code>localhost:31080</code> to the Pod running the registry.</li> </ol> <p>The <code>containerd</code> daemon should detect the change and re-configure itself. If you believe that's not occurring, then <code>systemctl restart containerd</code>.</p>"},{"location":"quickstart-kubernetes/#3-verify","title":"3) Verify","text":""},{"location":"quickstart-kubernetes/#tail-the-logs-on-the-ociregistry-pod","title":"Tail the logs on the Ociregistry pod:","text":"<pre><code>kubectl -n ociregistry logs -f -l app.kubernetes.io/name=ociregistry\n</code></pre>"},{"location":"quickstart-kubernetes/#observe-the-startup-logs","title":"Observe the startup logs","text":"<pre><code>time=\"2025-04-24T21:11:41Z\" level=info msg=\"loaded 0 manifest(s) from the file system in 102.136\u00b5s\"\n----------------------------------------------------------------------\nOCI Registry: pull-only, pull-through, caching OCI Distribution Server\nVersion: vZ.Z.Z, build date: 2025-04-23T00:35:28.79Z\nStarted: 2025-04-24 21:11:41.727569108 +0000 UTC (port 8080)\nRunning as (uid:gid) 65532:65532\nProcess id: 1\nTls: none\nCommand line: /ociregistry/server --config-file /var/ociregistry/config/registry-config.yaml serve\n----------------------------------------------------------------------\ntime=\"2025-04-24T21:11:41Z\" level=info msg=\"server is running\"\n</code></pre>"},{"location":"quickstart-kubernetes/#run-the-hello-world-container","title":"Run the <code>hello-world</code> container","text":"<pre><code>kubectl run hello-world --image docker.io/hello-world:latest &amp;&amp;\\\n  sleep 5s &amp;&amp;\\\n  kubectl logs hello-world\n</code></pre>"},{"location":"quickstart-kubernetes/#result","title":"Result","text":"<p><pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n</code></pre> (remainder redacted for brevity...)</p>"},{"location":"quickstart-kubernetes/#observe-the-new-ociregistry-log-entries","title":"Observe the new Ociregistry log entries","text":"<pre><code>time=\"2025-04-24T21:13:54Z\" level=info msg=\"pulling manifest from upstream: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=501.131286ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world@sha256:c41088499908a59aae84b0a49c70e86f4731e588a737f1637e73c8c09d995654\\\"\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"echo server GET:/v2/library/hello-world/manifests/sha256:c410884999?ns=docker.io status=200 latency=2.267101ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:54Z\" level=info msg=\"pulling manifest from upstream: \\\"docker.io/library/hello-world@sha256:03b62250a3cb1abd125271d393fc08bf0cc713391eda6b57c02d1ef85efcc25c\\\"\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/manifests/sha256:03b62250a3?ns=docker.io status=200 latency=612.467708ms host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/blobs/sha256:74cc54e27d?ns=docker.io status=200 latency=234.948\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:13:55Z\" level=info msg=\"echo server GET:/v2/library/hello-world/blobs/sha256:e6590344b1?ns=docker.io status=200 latency=232.113\u00b5s host=localhost:31080 ip=10.200.0.232\"\n</code></pre>"},{"location":"quickstart-kubernetes/#run-the-hello-world-container-again-with-a-different-name","title":"Run the <code>hello-world</code> container again with a different name","text":"<p>This time, also specify a pull policy to force <code>containerd</code> to pull the image:</p> <pre><code>kubectl run hello-world-2 --image docker.io/hello-world:latest\\\n  --image-pull-policy=Always\n</code></pre>"},{"location":"quickstart-kubernetes/#observe-new-ociregistry-log-entries","title":"Observe new Ociregistry log entries","text":"<p>These entries indicate that the Ociregistry server is serving from the image cache instead of re-pulling from DockerHub:</p> <pre><code>time=\"2025-04-24T21:34:35Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:35Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=204.413\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:34:36Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:36Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=358.099\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:34:51Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:34:51Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=313.859\u00b5s host=localhost:31080 ip=10.200.0.232\"\ntime=\"2025-04-24T21:35:09Z\" level=info msg=\"serving manifest from cache: \\\"docker.io/library/hello-world:latest\\\"\"\ntime=\"2025-04-24T21:35:09Z\" level=info msg=\"echo server HEAD:/v2/library/hello-world/manifests/latest?ns=docker.io status=200 latency=836.294\u00b5s host=localhost:31080 ip=10.200.0.232\"\n</code></pre>"},{"location":"quickstart-kubernetes/#summary","title":"Summary","text":"<p>In this quick start, you Helm-installed Ociregistry in the cluster bound to a NodePort. You configured all containerd instances in the cluster to mirror to the in-cluster registry. You verified that an initial pull causes the Ociregistry server to pull through to the upstream (Docker Hub in this case) and that subsequent image pulls by containerd were served by the Ociregistry server from its cache, avoiding any subsequent round trips to the upstream.</p>"},{"location":"rest-api/","title":"Administrative REST API","text":"<p>The following REST endpoints are supported for administration of the image cache. Note - the output of the commands in some cases is columnar. Pipe through <code>column -t</code> to columnize.</p>"},{"location":"rest-api/#cmdprune","title":"<code>/cmd/prune</code>","text":"<p>Prunes the in-memory cache and the file system while the server is running.</p> Query param Description <code>type</code> Valid values: <code>accessed</code>, <code>created</code>, <code>pattern</code>. <code>dur</code> A duration string. E.g.: <code>30d</code>. Valid time units are <code>d</code>=days, <code>m</code>=minutes, and <code>h</code>=hours.  If <code>type</code> is <code>accessed</code>, then images that have not been accessed within the duration are pruned. If <code>type</code> is <code>created</code>, then images created earlier than the duration ago are pruned. (I.e.: created more than 30 days ago.) If <code>type</code> is <code>pattern</code>, then <code>dur</code> is ignored. <code>expr</code> If <code>type</code> is <code>pattern</code>, then a manifest URL pattern like <code>calico</code>, else ignored. Multiple patterns can be separated by commas: <code>foo,bar</code> <code>count</code> Max manifests to prune. Defaults to <code>50</code>. <code>-1</code> means no limit. <code>dryRun</code> If <code>true</code> then logs messages but does not prune. Defaults to false, meaning: will prune by default. <p>Example:</p> <pre><code>curl -X DELETE \"http://hostname:8080/cmd/prune?type=created&amp;dur=10d&amp;count=50&amp;dryRun=true\"\n</code></pre> <p>Explanation: Prunes manifests created (initially downloaded) more than 10 days ago. Only prune a max of 50. Since dry run is true, doesn't actually prune - only show what prune would do.</p>"},{"location":"rest-api/#cmdimagelist","title":"<code>/cmd/image/list</code>","text":"<p>Lists image manifests, and the blobs that are referenced by the selected manifests.</p> Query param Description <code>pattern</code> Comma-separated go regex expressions of manifest URL(s). <code>digest</code> Digest (or substring) of any blob referenced by the image. (Not the manifest digest!) <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/image/list?pattern=docker.io&amp;count=10\"\n</code></pre></p> <p>Explanation: List a max of 10 image manifests with <code>docker.io</code> in the URL.</p>"},{"location":"rest-api/#cmdbloblist","title":"<code>/cmd/blob/list</code>","text":"<p>Lists blobs and ref counts.</p> Query param Description <code>substr</code> Digest (or substring) of a blob. <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/blob/list?substr=56aebe9b&amp;count=10\"\n</code></pre></p>"},{"location":"rest-api/#cmdmanifestlist","title":"<code>/cmd/manifest/list</code>","text":"<p>List manifests.</p> Query param Description <code>pattern</code> Comma-separated go regex expressions of manifest URLs. <code>count</code> Max number of manifests to return. Defaults to <code>50</code>. <code>-1</code> means no limit. <p>Example: <pre><code>curl \"http://hostname:8080/cmd/manifest/list?pattern=calico,cilium&amp;count=10\"\n</code></pre></p>"},{"location":"rest-api/#cmdstop","title":"<code>/cmd/stop</code>","text":"<p>Stops the server.</p>"},{"location":"systemd/","title":"Running Ociregistry as a systemd Service","text":"<p>The <code>systemd-service</code> directory has a systemd unit file for running the server as a systemd service:</p> <pre><code>[Unit]\nDescription=ociregistry\nDocumentation=https://github.com/aceeric/ociregistry\nAfter=network.target\n\n[Service]\nExecStart=/bin/ociregistry-server\\\n  --image-path=/var/lib/ociregistry\\\n  --log-level=info\\\n  serve\nType=simple\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>You can use the provided <code>manual-install</code> script in that directory to perform the installation.</p>"}]}